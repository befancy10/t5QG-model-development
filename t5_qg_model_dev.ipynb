{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orTdmICW31se"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DvvktoOUyFj"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L13eOOW-tFX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# T5-Base Question Generation Pipeline\n",
        "\n",
        "# Install required libraries\n",
        "!pip install pytorch_lightning scikit-learn matplotlib seaborn optuna sacrebleu rouge-score nltk==3.8.1 transformers>=4.41.0\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet tqdm==4.57.0\n",
        "!pip install --quiet evaluate openpyxl\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkSCa6s3UyFk"
      },
      "outputs": [],
      "source": [
        "!pip show sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfbIVOIn36dd"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "386l0sGq-vTo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import csv\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import AdamW\n",
        "from sklearn.utils import shuffle\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import optuna\n",
        "import nltk\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ML7LEO39ng"
      },
      "source": [
        "# Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87ErWGT5-wjd"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c91sMzV3_52"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0LrdG7m-x9e"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define project paths\n",
        "ROOT_PATH = '/content/drive/My Drive/.Skripsi'\n",
        "PROJECT_PATH = os.path.join(ROOT_PATH, 'HPO_Final')\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'dataset')\n",
        "MODEL_PATH = os.path.join(PROJECT_PATH, 'model')\n",
        "TOKENIZER_PATH = os.path.join(PROJECT_PATH, 'tokenizer')\n",
        "LOG_PATH = os.path.join(PROJECT_PATH, 'log')\n",
        "EVALUATION_PATH = os.path.join(PROJECT_PATH, 'evaluation')\n",
        "OPTUNA_PATH = os.path.join(PROJECT_PATH, 'optuna')\n",
        "\n",
        "# Create directories if missing\n",
        "os.makedirs(PROJECT_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(TOKENIZER_PATH, exist_ok=True)\n",
        "os.makedirs(LOG_PATH, exist_ok=True)\n",
        "os.makedirs(EVALUATION_PATH, exist_ok=True)\n",
        "os.makedirs(OPTUNA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4mhdPEy7oUj"
      },
      "outputs": [],
      "source": [
        "# Show project folder contents\n",
        "contents = os.listdir(PROJECT_PATH)\n",
        "print(f\"Contents of {PROJECT_PATH}:\")\n",
        "for item in contents:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIjZpaNj0W3h"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lua7wtMA-4An"
      },
      "outputs": [],
      "source": [
        "# Load Excel dataset and convert to CSV\n",
        "xlsx_input_file = os.path.join(ROOT_PATH, \"dataset_final.xlsx\")\n",
        "csv_output_file = os.path.join(ROOT_PATH, \"dataset_final.csv\")\n",
        "df_xlsx = pd.read_excel(xlsx_input_file)\n",
        "print(f\"Original dataset shape: {df_xlsx.shape}\")\n",
        "print(f\"Original columns: {list(df_xlsx.columns)}\")\n",
        "\n",
        "# Extract required columns\n",
        "required_columns = ['context', 'question', 'answers', 'question_type']\n",
        "missing_columns = [col for col in required_columns if col not in df_xlsx.columns]\n",
        "if missing_columns:\n",
        "    print(f\"Missing columns: {missing_columns}\")\n",
        "else:\n",
        "    print(\"All required columns present\")\n",
        "df_extracted = df_xlsx[required_columns].copy()\n",
        "print(f\"Extracted dataset shape: {df_extracted.shape}\")\n",
        "print(f\"Final columns: {list(df_extracted.columns)}\")\n",
        "\n",
        "# Check missing and empty values\n",
        "for col in df_extracted.columns:\n",
        "    missing_count = df_extracted[col].isna().sum()\n",
        "    empty_count = (df_extracted[col].astype(str).str.strip() == '').sum()\n",
        "    print(f\"  {col}: {missing_count} missing, {empty_count} empty strings\")\n",
        "\n",
        "# Save extracted CSV\n",
        "df_extracted.to_csv(csv_output_file, index=False, quoting=csv.QUOTE_ALL)\n",
        "print(f\"Saved extracted data to: {csv_output_file}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nSample of extracted data:\")\n",
        "print(\"Context (first 100 chars):\", str(df_extracted['context'].iloc[0])[:100] + \"...\")\n",
        "print(\"Question:\", df_extracted['question'].iloc[0])\n",
        "print(\"Answer:\", df_extracted['answers'].iloc[0])\n",
        "print(\"Question Type:\", df_extracted['question_type'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofV49ZMY-99n"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clean excessive quotes\n",
        "input_file_path = os.path.join(ROOT_PATH, \"dataset_final.csv\")\n",
        "output_file_quoted_path = os.path.join(ROOT_PATH, \"dataset_final_quoted.csv\")\n",
        "df = pd.read_csv(input_file_path, quotechar='\"', dtype=str, keep_default_na=False)\n",
        "print(f\"Loaded dataset with shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "# Remove extra quotes\n",
        "def remove_extra_quotes(text):\n",
        "    return text.strip('\"')\n",
        "\n",
        "print(\"Removing excessive quotes...\")\n",
        "df[\"context\"] = df[\"context\"].apply(remove_extra_quotes)\n",
        "df[\"answers\"] = df[\"answers\"].apply(remove_extra_quotes)\n",
        "df[\"question\"] = df[\"question\"].apply(remove_extra_quotes)\n",
        "df.to_csv(output_file_quoted_path, index=False, quoting=csv.QUOTE_ALL)\n",
        "print(f\"Saved quoted-cleaned file to: {output_file_quoted_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sm2J5cy_Aa1"
      },
      "outputs": [],
      "source": [
        "# Step 2: Clean and normalize text\n",
        "input_csv = output_file_quoted_path\n",
        "output_csv = os.path.join(ROOT_PATH, \"dataset_final_cleaned.csv\")\n",
        "\n",
        "# Normalize text\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    return text.strip()\n",
        "\n",
        "columns_to_clean = [\"context\", \"question\", \"answers\"]\n",
        "df = pd.read_csv(input_csv, dtype=str, keep_default_na=False)\n",
        "print(f\"Processing {len(df)} rows for text normalization...\")\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    if col in df.columns:\n",
        "        print(f\"Cleaning column: {col}\")\n",
        "        df[col] = df[col].apply(clean_text)\n",
        "    else:\n",
        "        print(f\"WARNING: Column '{col}' not found\")\n",
        "\n",
        "df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n",
        "print(f\"Saved cleaned CSV to: {output_csv}\")\n",
        "\n",
        "# Show sample of cleaned data\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(\"Context (first 100 chars):\", df['context'].iloc[0][:100] + \"...\")\n",
        "print(\"Question:\", df['question'].iloc[0])\n",
        "print(\"Answer:\", df['answers'].iloc[0])\n",
        "print(\"Question Type:\", df['question_type'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkjdFrOxhHN4"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87T1EFeD_CU2"
      },
      "outputs": [],
      "source": [
        "# Load cleaned dataset\n",
        "dataset_path = os.path.join(ROOT_PATH, 'dataset_final_cleaned.csv')\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Show dataset info\n",
        "print(f\"Dataset shape before stratified split: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqK7DyS_vrKl"
      },
      "outputs": [],
      "source": [
        "# Show distribution before split\n",
        "print(\"\\nDistribution before split:\")\n",
        "print(\"Question Type distribution:\")\n",
        "question_type_dist = df['question_type'].value_counts().sort_index()\n",
        "total_samples = len(df)\n",
        "print(\"=\"*50)\n",
        "for qtype, count in question_type_dist.items():\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"{qtype:8}: {count:4} samples ({percentage:5.2f}%)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Minimum samples check for stratification\n",
        "min_samples = question_type_dist.min()\n",
        "print(f\"\\nMinimum samples per question type: {min_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LX8OAjDsYor"
      },
      "outputs": [],
      "source": [
        "if min_samples < 2:\n",
        "    print(\"WARNING: Some question types have less than 2 samples. Cannot perform stratified sampling.\")\n",
        "    print(\"Falling back to regular random sampling.\")\n",
        "    # Stratified split into train, validation, and test sets (80:10:10) - Regular split\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "else:\n",
        "    print(\"All question types have sufficient samples. Proceeding with stratified sampling.\")\n",
        "    stratified = True\n",
        "    # Stratified split into train, validation, and test sets (80:10:10)\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['question_type']\n",
        "    )\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=0.5,\n",
        "        random_state=42,\n",
        "        stratify=temp_df['question_type']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKNEcvukwJzU"
      },
      "outputs": [],
      "source": [
        "# DETAILED LOGGING OF QUESTION TYPE DISTRIBUTION AFTER SPLIT\n",
        "# (BEFORE removing question_type column)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED QUESTION TYPE DISTRIBUTION AFTER SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate distributions for each split\n",
        "train_dist = train_df['question_type'].value_counts().sort_index()\n",
        "val_dist = val_df['question_type'].value_counts().sort_index()\n",
        "test_dist = test_df['question_type'].value_counts().sort_index()\n",
        "\n",
        "# Get all unique question types\n",
        "all_qtypes = sorted(df['question_type'].unique())\n",
        "\n",
        "# Create summary table\n",
        "print(f\"{'Question Type':<12} {'Original':<12} {'Train':<12} {'Validation':<12} {'Test':<12}\")\n",
        "print(f\"{'':12} {'Count (%)':<12} {'Count (%)':<12} {'Count (%)':<12} {'Count (%)':<12}\")\n",
        "print(\"-\" * 72)\n",
        "\n",
        "for qtype in all_qtypes:\n",
        "    orig_count = question_type_dist.get(qtype, 0)\n",
        "    orig_pct = (orig_count / total_samples) * 100\n",
        "\n",
        "    train_count = train_dist.get(qtype, 0)\n",
        "    train_pct = (train_count / len(train_df)) * 100\n",
        "\n",
        "    val_count = val_dist.get(qtype, 0)\n",
        "    val_pct = (val_count / len(val_df)) * 100\n",
        "\n",
        "    test_count = test_dist.get(qtype, 0)\n",
        "    test_pct = (test_count / len(test_df)) * 100\n",
        "\n",
        "    print(f\"{qtype:<12} {orig_count:4}({orig_pct:5.1f}%) {train_count:4}({train_pct:5.1f}%) {val_count:4}({val_pct:5.1f}%) {test_count:4}({test_pct:5.1f}%)\")\n",
        "\n",
        "print(\"-\" * 72)\n",
        "print(f\"{'TOTAL':<12} {total_samples:4}({100.0:5.1f}%) {len(train_df):4}({100.0:5.1f}%) {len(val_df):4}({100.0:5.1f}%) {len(test_df):4}({100.0:5.1f}%)\")\n",
        "\n",
        "# Verify stratification quality (only if stratified sampling was used)\n",
        "if stratified:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STRATIFICATION QUALITY CHECK\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Checking if proportions are maintained across splits...\")\n",
        "\n",
        "    max_deviation = 0\n",
        "    for qtype in all_qtypes:\n",
        "        if qtype in question_type_dist:\n",
        "            orig_pct = (question_type_dist[qtype] / total_samples) * 100\n",
        "            train_pct = (train_dist.get(qtype, 0) / len(train_df)) * 100\n",
        "            val_pct = (val_dist.get(qtype, 0) / len(val_df)) * 100\n",
        "            test_pct = (test_dist.get(qtype, 0) / len(test_df)) * 100\n",
        "\n",
        "            train_dev = abs(orig_pct - train_pct)\n",
        "            val_dev = abs(orig_pct - val_pct)\n",
        "            test_dev = abs(orig_pct - test_pct)\n",
        "\n",
        "            max_dev = max(train_dev, val_dev, test_dev)\n",
        "            max_deviation = max(max_deviation, max_dev)\n",
        "\n",
        "            print(f\"{qtype}: Original {orig_pct:.1f}% | Deviations: Train ±{train_dev:.1f}%, Val ±{val_dev:.1f}%, Test ±{test_dev:.1f}%\")\n",
        "\n",
        "    print(f\"\\nMaximum deviation from original proportions: ±{max_deviation:.1f}%\")\n",
        "    if max_deviation < 2.0:\n",
        "        print(\" EXCELLENT: Stratification maintained proportions very well\")\n",
        "    elif max_deviation < 5.0:\n",
        "        print(\" GOOD: Stratification maintained proportions adequately\")\n",
        "    else:\n",
        "        print(\" WARNING: Some proportions deviated significantly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8TTAFEg_Kwu"
      },
      "outputs": [],
      "source": [
        "# Keep only required columns\n",
        "core_columns = ['context', 'answers', 'question']\n",
        "train_df = train_df[core_columns].copy()\n",
        "val_df = val_df[core_columns].copy()\n",
        "test_df = test_df[core_columns].copy()\n",
        "\n",
        "# Shuffle the dataframes\n",
        "train_df = shuffle(train_df, random_state=42)\n",
        "val_df = shuffle(val_df, random_state=42)\n",
        "test_df = shuffle(test_df, random_state=42)\n",
        "\n",
        "# Save processed dataset\n",
        "processed_dataset_path = os.path.join(DATA_PATH, 'processed_dataset.csv')\n",
        "df[core_columns].to_csv(processed_dataset_path, index=False, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "# Save splits\n",
        "train_df_path = os.path.join(DATA_PATH, 'train_df.csv')\n",
        "val_df_path = os.path.join(DATA_PATH, 'val_df.csv')\n",
        "test_df_path = os.path.join(DATA_PATH, 'test_df.csv')\n",
        "\n",
        "train_df.to_csv(train_df_path, index=False, quoting=csv.QUOTE_ALL)\n",
        "val_df.to_csv(val_df_path, index=False, quoting=csv.QUOTE_ALL)\n",
        "test_df.to_csv(test_df_path, index=False, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "# Reload dataset\n",
        "train_df = pd.read_csv(train_df_path)\n",
        "val_df = pd.read_csv(val_df_path)\n",
        "test_df = pd.read_csv(test_df_path)\n",
        "\n",
        "# Count and show dataset distribution\n",
        "total_len = len(train_df) + len(val_df) + len(test_df)\n",
        "train_len = len(train_df)\n",
        "val_len = len(val_df)\n",
        "test_len = len(test_df)\n",
        "\n",
        "train_percentage = (train_len / total_len) * 100\n",
        "val_percentage = (val_len / total_len) * 100\n",
        "test_percentage = (test_len / total_len) * 100\n",
        "\n",
        "print(f\"\\n=== FINAL DATASET SPLIT SUMMARY ===\")\n",
        "print(f\"Train length: {train_len} samples ({train_percentage:.2f}%)\")\n",
        "print(f\"Validation length: {val_len} samples ({val_percentage:.2f}%)\")\n",
        "print(f\"Test length: {test_len} samples ({test_percentage:.2f}%)\")\n",
        "print(f\"Total: {total_len} samples\")\n",
        "\n",
        "if stratified:\n",
        "    print(f\"\\Stratified random sampling completed successfully\")\n",
        "    print(f\"Proportional distribution maintained across question_type\")\n",
        "else:\n",
        "    print(f\"\\Regular random sampling completed\")\n",
        "    print(f\"Note: Stratification was not possible due to insufficient samples in some categories\")\n",
        "\n",
        "print(f\"Final datasets contain only core columns: {core_columns}\")\n",
        "print(f\"All split files saved to: {DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5OhAWC_LNn"
      },
      "outputs": [],
      "source": [
        "## Load T5-base pretrained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "TOKENIZER = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "print(f\"Model loaded: T5-base\")\n",
        "print(f\"Model parameters: {MODEL.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-nmYqUm_TC-"
      },
      "outputs": [],
      "source": [
        "# Analyze token lengths for each column\n",
        "def analyze_lengths(df, tokenizer, column):\n",
        "    lengths = [len(tokenizer.encode(str(row[column]))) for _, row in df.iterrows()]\n",
        "    print(f\"{column} - Max length: {max(lengths)}, Avg length: {sum(lengths)/len(lengths):.2f}\")\n",
        "\n",
        "analyze_lengths(train_df, TOKENIZER, 'context')\n",
        "analyze_lengths(train_df, TOKENIZER, 'answers')\n",
        "analyze_lengths(train_df, TOKENIZER, 'question')\n",
        "\n",
        "# Combine splits for sequence length analysis\n",
        "full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "input_seq_lengths, target_seq_lengths = [], []\n",
        "\n",
        "for _, row in full_df.iterrows():\n",
        "    input_text = f\"context: {row['context']} answers: {row['answers']}\"\n",
        "    input_tokens = TOKENIZER.encode(input_text, add_special_tokens=True)\n",
        "    input_seq_lengths.append(len(input_tokens))\n",
        "\n",
        "    target_text = f\"question: {row['question']}\"\n",
        "    target_tokens = TOKENIZER.encode(target_text, add_special_tokens=True)\n",
        "    target_seq_lengths.append(len(target_tokens))\n",
        "\n",
        "# Compute percentiles for input sequence lengths\n",
        "input_max_len_90 = int(np.percentile(input_seq_lengths, 90))\n",
        "input_max_len_95 = int(np.percentile(input_seq_lengths, 95))\n",
        "input_max_len_99 = int(np.percentile(input_seq_lengths, 99))\n",
        "print(f\"Input Sequence Lengths (context + answers):\")\n",
        "print(f\"90th percentile: {input_max_len_90}\")\n",
        "print(f\"95th percentile: {input_max_len_95}\")\n",
        "print(f\"99th percentile: {input_max_len_99}\")\n",
        "print(f\"Max length: {max(input_seq_lengths)}\")\n",
        "print(f\"Average length: {np.mean(input_seq_lengths):.2f}\")\n",
        "\n",
        "# Compute percentiles for target sequence lengths\n",
        "target_max_len_90 = int(np.percentile(target_seq_lengths, 90))\n",
        "target_max_len_95 = int(np.percentile(target_seq_lengths, 95))\n",
        "target_max_len_99 = int(np.percentile(target_seq_lengths, 99))\n",
        "print(f\"\\nTarget Sequence Lengths (question):\")\n",
        "print(f\"90th percentile: {target_max_len_90}\")\n",
        "print(f\"95th percentile: {target_max_len_95}\")\n",
        "print(f\"99th percentile: {target_max_len_99}\")\n",
        "print(f\"Max length: {max(target_seq_lengths)}\")\n",
        "print(f\"Average length: {np.mean(target_seq_lengths):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8GSTxXUyFm"
      },
      "source": [
        "# Class Dasataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYsUHKGd_la3"
      },
      "outputs": [],
      "source": [
        "# Use max lengths for training\n",
        "input_max_len = max(input_seq_lengths)\n",
        "target_max_len = max(target_seq_lengths)\n",
        "TOKENIZER = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "print(f\"Using sequence lengths - Input: {input_max_len}, Target: {target_max_len}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Custom dataset for T5\n",
        "class QuestionGenerationDataset(Dataset):\n",
        "    def __init__(self, tokenizer, filepath, max_len_inp=input_max_len, max_len_out=target_max_len):\n",
        "        self.path = filepath\n",
        "        self.passage_column = \"context\"\n",
        "        self.answers = \"answers\"\n",
        "        self.question = \"question\"\n",
        "        self.data = pd.read_csv(self.path)\n",
        "        self.max_len_input = max_len_inp\n",
        "        self.max_len_output = max_len_out\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs, self.targets = [], []\n",
        "        self.skippedcount = 0\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
        "        labels = copy.deepcopy(target_ids)\n",
        "        labels[labels == 0] = -100\n",
        "        return {\n",
        "            \"source_ids\": source_ids,\n",
        "            \"source_mask\": src_mask,\n",
        "            \"target_ids\": target_ids,\n",
        "            \"target_mask\": target_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "    def _build(self):\n",
        "        print(f\"Building dataset from: {self.path}\")\n",
        "        print(f\"Dataset shape: {self.data.shape}\")\n",
        "        print(f\"Columns: {list(self.data.columns)}\")\n",
        "\n",
        "        for idx in tqdm(range(len(self.data)), desc=\"Processing samples\"):\n",
        "            passage = self.data.loc[idx, self.passage_column]\n",
        "            answers = self.data.loc[idx, self.answers]\n",
        "            target = self.data.loc[idx, self.question]\n",
        "\n",
        "            input_ = f\"context: {passage} answers: {answers}\"\n",
        "            target = f\"question: {str(target)}\"\n",
        "\n",
        "            test_input_encoding = self.tokenizer.encode_plus(input_, truncation=False, return_tensors=\"pt\")\n",
        "            length_of_input_encoding = len(test_input_encoding['input_ids'][0])\n",
        "            if length_of_input_encoding > self.max_len_input:\n",
        "                self.skippedcount += 1\n",
        "                continue\n",
        "\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_],\n",
        "                max_length=self.max_len_input,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target],\n",
        "                max_length=self.max_len_output,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)\n",
        "\n",
        "        print(f\"  Dataset built successfully\")\n",
        "        print(f\"  Total samples processed: {len(self.data)}\")\n",
        "        print(f\"  Samples included: {len(self.inputs)}\")\n",
        "        print(f\"  Samples skipped (too long): {self.skippedcount}\")\n",
        "        print(f\"  Inclusion rate: {len(self.inputs)/len(self.data)*100:.2f}%\")\n",
        "\n",
        "# Build train, validation, and test datasets\n",
        "train_dataset = QuestionGenerationDataset(TOKENIZER, train_df_path)\n",
        "val_dataset   = QuestionGenerationDataset(TOKENIZER, val_df_path)\n",
        "test_dataset  = QuestionGenerationDataset(TOKENIZER, test_df_path)\n",
        "\n",
        "# Show dataset statistics\n",
        "total_len = len(train_dataset) + len(val_dataset) + len(test_dataset)\n",
        "train_len, val_len, test_len = len(train_dataset), len(val_dataset), len(test_dataset)\n",
        "print(f\"\\n=== FINAL DATASET STATISTICS ===\")\n",
        "print(f\"Train dataset length: {train_len}\")\n",
        "print(f\"Validation dataset length: {val_len}\")\n",
        "print(f\"Test dataset length: {test_len}\")\n",
        "print(f\"Total dataset length: {total_len}\")\n",
        "print(f\"Train percentage: {(train_len/total_len)*100:.2f}%\")\n",
        "print(f\"Validation percentage: {(val_len/total_len)*100:.2f}%\")\n",
        "print(f\"Test percentage: {(test_len/total_len)*100:.2f}%\")\n",
        "\n",
        "# Show skipped samples summary\n",
        "total_skipped = train_dataset.skippedcount + val_dataset.skippedcount + test_dataset.skippedcount\n",
        "original_total = len(pd.read_csv(train_df_path)) + len(pd.read_csv(val_df_path)) + len(pd.read_csv(test_df_path))\n",
        "\n",
        "if total_skipped > 0:\n",
        "    print(f\"\\nSKIPPED SAMPLES SUMMARY:\")\n",
        "    print(f\"Train skipped: {train_dataset.skippedcount}\")\n",
        "    print(f\"Validation skipped: {val_dataset.skippedcount}\")\n",
        "    print(f\"Test skipped: {test_dataset.skippedcount}\")\n",
        "    print(f\"Total skipped: {total_skipped}\")\n",
        "    print(f\"Skip rate: {total_skipped/original_total*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"\\nNo samples skipped - all data within sequence length limits\")\n",
        "\n",
        "print(f\"\\nDataset creation completed successfully\")\n",
        "print(f\"Ready for training with sequence lengths: {input_max_len}/{target_max_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRK_rMHxGdPo"
      },
      "source": [
        "# HPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgDjwUPf3OJh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Fungsi Analisis Data HPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E28ZgaFA3TSd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import optuna\n",
        "import gc\n",
        "\n",
        "# 1. Export basic trial data to CSV\n",
        "def export_trials_to_csv(study, base_path=None):\n",
        "    # Export all trial data to CSV only\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "\n",
        "    df = study.trials_dataframe()\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = os.path.join(base_path, f'optuna_trials_{timestamp}.csv')\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Trial data exported to CSV: {csv_filename}\")\n",
        "    return df\n",
        "\n",
        "# 2. Create detailed TPE analysis\n",
        "def create_detailed_tpe_analysis(study, base_path=None):\n",
        "    # Create detailed analysis of TPE decision making process\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "\n",
        "    trials_data = []\n",
        "    for i, trial in enumerate(study.trials):\n",
        "        trial_data = {\n",
        "            'trial_number': trial.number,\n",
        "            'objective_value': trial.value,\n",
        "            'bleu_score': -trial.value if trial.value is not None else None,\n",
        "            'state': trial.state.name,\n",
        "            'duration_seconds': trial.duration.total_seconds() if trial.duration else None,\n",
        "            'datetime_start': trial.datetime_start,\n",
        "            'datetime_complete': trial.datetime_complete\n",
        "        }\n",
        "\n",
        "        for param_name, param_value in trial.params.items():\n",
        "            trial_data[f'param_{param_name}'] = param_value\n",
        "        for attr_name, attr_value in trial.user_attrs.items():\n",
        "            trial_data[attr_name] = attr_value\n",
        "\n",
        "        if i > 0:\n",
        "            completed_trials = [t for t in study.trials[:i] if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
        "            if completed_trials:\n",
        "                values = [t.value for t in completed_trials]\n",
        "                threshold = np.percentile(values, 25)\n",
        "                trial_data['is_good_region'] = trial.value <= threshold if trial.value is not None else None\n",
        "                trial_data['rank_percentile'] = (sorted(values + [trial.value]).index(trial.value) + 1) / len(values + [trial.value]) * 100 if trial.value is not None else None\n",
        "            else:\n",
        "                trial_data['is_good_region'] = None\n",
        "                trial_data['rank_percentile'] = None\n",
        "        else:\n",
        "            trial_data['is_good_region'] = None\n",
        "            trial_data['rank_percentile'] = None\n",
        "\n",
        "        completed_values = [t.value for t in study.trials[:i+1] if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
        "        if completed_values:\n",
        "            best_value = min(completed_values)\n",
        "            trial_data['best_bleu_so_far'] = -best_value\n",
        "            trial_data['is_improvement'] = trial.value == best_value if trial.value is not None else False\n",
        "        else:\n",
        "            trial_data['best_bleu_so_far'] = None\n",
        "            trial_data['is_improvement'] = False\n",
        "\n",
        "        trials_data.append(trial_data)\n",
        "\n",
        "    detailed_df = pd.DataFrame(trials_data)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = os.path.join(base_path, f'detailed_tpe_analysis_{timestamp}.csv')\n",
        "    detailed_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Detailed TPE analysis exported to CSV: {csv_filename}\")\n",
        "    return detailed_df\n",
        "\n",
        "# 3. Export parameter importance analysis\n",
        "def export_parameter_importance(study, base_path=None):\n",
        "    # Calculate and export parameter importance\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "    try:\n",
        "        importance = optuna.importance.get_param_importances(study)\n",
        "        importance_df = pd.DataFrame([\n",
        "            {'parameter': param, 'importance': imp}\n",
        "            for param, imp in importance.items()\n",
        "        ]).sort_values('importance', ascending=False)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filename = os.path.join(base_path, f'parameter_importance_{timestamp}.csv')\n",
        "        importance_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Parameter importance exported to CSV: {csv_filename}\")\n",
        "        return importance_df\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate parameter importance: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. Analyze TPE distributions per parameter\n",
        "def analyze_tpe_distributions_per_param(study, param_name):\n",
        "    # Analyze TPE distributions for a specific parameter\n",
        "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
        "    if len(completed_trials) < 2:\n",
        "        return None\n",
        "\n",
        "    sorted_trials = sorted(completed_trials, key=lambda x: x.value)\n",
        "    n_good = max(1, len(sorted_trials) // 4)\n",
        "    good_trials, poor_trials = sorted_trials[:n_good], sorted_trials[n_good:]\n",
        "\n",
        "    good_values = [t.params[param_name] for t in good_trials if param_name in t.params]\n",
        "    poor_values = [t.params[param_name] for t in poor_trials if param_name in t.params]\n",
        "\n",
        "    stats = {\n",
        "        'parameter': param_name,\n",
        "        'total_trials': len(completed_trials),\n",
        "        'good_trials_count': len(good_values),\n",
        "        'poor_trials_count': len(poor_values),\n",
        "        'good_mean': np.mean(good_values) if good_values else None,\n",
        "        'good_std': np.std(good_values) if good_values else None,\n",
        "        'good_min': np.min(good_values) if good_values else None,\n",
        "        'good_max': np.max(good_values) if good_values else None,\n",
        "        'poor_mean': np.mean(poor_values) if poor_values else None,\n",
        "        'poor_std': np.std(poor_values) if poor_values else None,\n",
        "        'poor_min': np.min(poor_values) if poor_values else None,\n",
        "        'poor_max': np.max(poor_values) if poor_values else None,\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def export_all_tpe_distributions(study, base_path=None):\n",
        "    # Export TPE distribution analysis for all parameters\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "\n",
        "    all_params = set()\n",
        "    for trial in study.trials:\n",
        "        all_params.update(trial.params.keys())\n",
        "\n",
        "    distribution_stats = []\n",
        "    for param in all_params:\n",
        "        stats = analyze_tpe_distributions_per_param(study, param)\n",
        "        if stats:\n",
        "            distribution_stats.append(stats)\n",
        "\n",
        "    if distribution_stats:\n",
        "        dist_df = pd.DataFrame(distribution_stats)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filename = os.path.join(base_path, f'tpe_distributions_analysis_{timestamp}.csv')\n",
        "        dist_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"TPE distributions analysis exported to CSV: {csv_filename}\")\n",
        "        return dist_df\n",
        "    else:\n",
        "        print(\"No distribution analysis data available\")\n",
        "        return None\n",
        "\n",
        "# 5. Generate comprehensive optimization report\n",
        "def generate_optimization_report(study, base_path=None):\n",
        "    # Generate comprehensive text report of optimization process\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = os.path.join(base_path, f'optimization_report_{timestamp}.txt')\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"T5 QUESTION GENERATION - OPTUNA TPE OPTIMIZATION REPORT\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Study Direction: {study.direction.name}\\n\")\n",
        "        f.write(f\"Total Trials: {len(study.trials)}\\n\")\n",
        "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "        f.write(f\"Completed Trials: {len(completed_trials)}\\n\")\n",
        "        failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]\n",
        "        f.write(f\"Failed Trials: {len(failed_trials)}\\n\\n\")\n",
        "\n",
        "        if study.best_trial:\n",
        "            f.write(\"BEST RESULT:\\n\")\n",
        "            f.write(f\"Best BLEU-4 Score: {-study.best_value:.4f}%\\n\")\n",
        "            f.write(f\"Best Trial Number: {study.best_trial.number}\\n\")\n",
        "            f.write(\"Best Hyperparameters:\\n\")\n",
        "            for param, value in study.best_params.items():\n",
        "                f.write(f\"  {param}: {value}\\n\")\n",
        "            f.write(\"\\nBest Trial Additional Metrics:\\n\")\n",
        "            for attr_name, attr_value in study.best_trial.user_attrs.items():\n",
        "                f.write(f\"  {attr_name}: {attr_value}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        try:\n",
        "            importance = optuna.importance.get_param_importances(study)\n",
        "            f.write(\"PARAMETER IMPORTANCE:\\n\")\n",
        "            for param, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True):\n",
        "                f.write(f\"  {param}: {imp:.4f}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "        except:\n",
        "            f.write(\"PARAMETER IMPORTANCE: Could not calculate\\n\\n\")\n",
        "\n",
        "        f.write(\"TPE ANALYSIS SUMMARY:\\n\")\n",
        "        all_params = set()\n",
        "        for trial in study.trials:\n",
        "            all_params.update(trial.params.keys())\n",
        "        for param in all_params:\n",
        "            stats = analyze_tpe_distributions_per_param(study, param)\n",
        "            if stats and stats['good_mean'] is not None and stats['poor_mean'] is not None:\n",
        "                f.write(f\"\\n{param}:\\n\")\n",
        "                f.write(f\"  Good trials mean: {stats['good_mean']}\\n\")\n",
        "                f.write(f\"  Poor trials mean: {stats['poor_mean']}\\n\")\n",
        "                f.write(f\"  Difference: {abs(stats['good_mean'] - stats['poor_mean']):.4f}\\n\")\n",
        "                f.write(f\"  Good trials count: {stats['good_trials_count']}\\n\")\n",
        "                f.write(f\"  Poor trials count: {stats['poor_trials_count']}\\n\")\n",
        "\n",
        "        f.write(\"\\nTOP 10 TRIALS:\\n\")\n",
        "        f.write(\"Rank | Trial | BLEU-4  | LR      | Batch | Weight Decay | Train Loss | Val Loss\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "        successful_trials = [(t, -t.value) for t in completed_trials if t.value is not None]\n",
        "        successful_trials.sort(key=lambda x: x[1], reverse=True)\n",
        "        for i, (trial, bleu) in enumerate(successful_trials[:10], 1):\n",
        "            lr = trial.params.get('learning_rate', 'N/A')\n",
        "            bs = trial.params.get('batch_size', 'N/A')\n",
        "            wd = trial.params.get('weight_decay', 'N/A')\n",
        "            train_loss = trial.user_attrs.get('final_train_loss', 'N/A')\n",
        "            val_loss = trial.user_attrs.get('final_val_loss', 'N/A')\n",
        "            f.write(f\"{i:4d} | {trial.number:5d} | {bleu:6.2f}% | {lr} | {bs:5} | {wd} | {train_loss} | {val_loss}\\n\")\n",
        "    print(f\"Comprehensive optimization report saved to: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# 6. Export performance summary statistics\n",
        "def export_performance_summary(study, base_path=None):\n",
        "    # Export summary statistics of optimization performance\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
        "    if not completed_trials:\n",
        "        print(\"No completed trials for performance summary\")\n",
        "        return None\n",
        "\n",
        "    bleu_scores = [-t.value for t in completed_trials]\n",
        "    summary_stats = {\n",
        "        'metric': ['BLEU-4 Score'],\n",
        "        'count': [len(bleu_scores)],\n",
        "        'mean': [np.mean(bleu_scores)],\n",
        "        'std': [np.std(bleu_scores)],\n",
        "        'min': [np.min(bleu_scores)],\n",
        "        'max': [np.max(bleu_scores)],\n",
        "        'median': [np.median(bleu_scores)],\n",
        "        'q25': [np.percentile(bleu_scores, 25)],\n",
        "        'q75': [np.percentile(bleu_scores, 75)]\n",
        "    }\n",
        "\n",
        "    meteor_scores = [t.user_attrs.get('meteor_score') for t in completed_trials if 'meteor_score' in t.user_attrs]\n",
        "    if meteor_scores:\n",
        "        meteor_scores = [s for s in meteor_scores if s is not None]\n",
        "        if meteor_scores:\n",
        "            summary_stats['metric'].append('METEOR Score')\n",
        "            summary_stats['count'].append(len(meteor_scores))\n",
        "            summary_stats['mean'].append(np.mean(meteor_scores))\n",
        "            summary_stats['std'].append(np.std(meteor_scores))\n",
        "            summary_stats['min'].append(np.min(meteor_scores))\n",
        "            summary_stats['max'].append(np.max(meteor_scores))\n",
        "            summary_stats['median'].append(np.median(meteor_scores))\n",
        "            summary_stats['q25'].append(np.percentile(meteor_scores, 25))\n",
        "            summary_stats['q75'].append(np.percentile(meteor_scores, 75))\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_stats)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = os.path.join(base_path, f'performance_summary_{timestamp}.csv')\n",
        "    summary_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Performance summary exported to CSV: {csv_filename}\")\n",
        "    return summary_df\n",
        "\n",
        "# 7. Create trial monitoring callback\n",
        "def create_monitoring_callback(base_path=None, save_every=5):\n",
        "    # Create callback for real-time monitoring during optimization\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "    def monitoring_callback(study, trial):\n",
        "        if trial.number % save_every == 0 and trial.number > 0:\n",
        "            print(f\"\\n=== MONITORING UPDATE - Trial {trial.number} ===\")\n",
        "            if study.best_trial:\n",
        "                best_bleu = -study.best_value\n",
        "                print(f\"Current Best BLEU-4: {best_bleu:.4f}%\")\n",
        "                print(f\"Best Trial: {study.best_trial.number}\")\n",
        "                print(f\"Best Parameters: {study.best_params}\")\n",
        "            recent_trials = [t for t in study.trials[-save_every:] if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None]\n",
        "            if recent_trials:\n",
        "                recent_bleus = [-t.value for t in recent_trials]\n",
        "                print(f\"Recent {len(recent_trials)} trials BLEU-4: {np.mean(recent_bleus):.4f}% ± {np.std(recent_bleus):.4f}%\")\n",
        "            try:\n",
        "                intermediate_df = study.trials_dataframe()\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                filename = os.path.join(base_path, f'intermediate_results_trial_{trial.number}_{timestamp}.csv')\n",
        "                intermediate_df.to_csv(filename, index=False)\n",
        "                print(f\"Intermediate results saved: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not save intermediate results: {e}\")\n",
        "            print(\"=\" * 50)\n",
        "    return monitoring_callback\n",
        "\n",
        "# 8. Master analysis function - run all analyses\n",
        "def run_complete_hpo_analysis(study, base_path=None):\n",
        "    # Run all analysis functions in sequence\n",
        "    if base_path is None:\n",
        "        base_path = LOG_PATH if 'LOG_PATH' in globals() else './'\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING COMPLETE HPO ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    try:\n",
        "        print(\"\\n1. Exporting basic trial data...\")\n",
        "        trials_df = export_trials_to_csv(study, base_path)\n",
        "        print(\"\\n2. Creating detailed TPE analysis...\")\n",
        "        detailed_df = create_detailed_tpe_analysis(study, base_path)\n",
        "        print(\"\\n3. Calculating parameter importance...\")\n",
        "        importance_df = export_parameter_importance(study, base_path)\n",
        "        print(\"\\n4. Analyzing TPE distributions...\")\n",
        "        distributions_df = export_all_tpe_distributions(study, base_path)\n",
        "        print(\"\\n5. Generating comprehensive report...\")\n",
        "        report_file = generate_optimization_report(study, base_path)\n",
        "        print(\"\\n6. Creating performance summary...\")\n",
        "        summary_df = export_performance_summary(study, base_path)\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"COMPLETE HPO ANALYSIS FINISHED\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"All files saved to: {base_path}\")\n",
        "        return {\n",
        "            'trials_df': trials_df,\n",
        "            'detailed_df': detailed_df,\n",
        "            'importance_df': importance_df,\n",
        "            'distributions_df': distributions_df,\n",
        "            'summary_df': summary_df,\n",
        "            'report_file': report_file\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error during analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ISg2S2IKUyFn"
      },
      "source": [
        "## HPO nya tanpa Dropout Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "f4c5kw_FUyFn"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Optimization Optuna - Clean Version\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from optuna.pruners import MedianPruner\n",
        "from evaluate import load\n",
        "import sacrebleu\n",
        "\n",
        "# Global variable to track best BLEU score\n",
        "current_best_bleu = 0.0\n",
        "\n",
        "def cleanup_memory(*objects):\n",
        "    \"\"\"Clean up GPU memory and objects\"\"\"\n",
        "    for obj in objects:\n",
        "        del obj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def compute_bleu_score_sacrebleu(references, predictions):\n",
        "    \"\"\"\n",
        "    Compute BLEU score using sacreBLEU (corpus-level)\n",
        "    References should be a list of strings\n",
        "    Predictions should be a list of strings\n",
        "    \"\"\"\n",
        "    return sacrebleu.corpus_bleu(predictions, [references]).score\n",
        "\n",
        "def compute_bleu_score_identical_to_baseline(references, predictions):\n",
        "    \"\"\"\n",
        "    Compute BLEU score using Hugging Face evaluate library\n",
        "    This is more stable and compatible with newer Python versions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load BLEU metric from Hugging Face evaluate\n",
        "        bleu_metric = load(\"bleu\")\n",
        "\n",
        "        # Format references as list of lists (required by evaluate library)\n",
        "        formatted_refs = [[ref] for ref in references]\n",
        "\n",
        "        # Compute BLEU score\n",
        "        result = bleu_metric.compute(\n",
        "            predictions=predictions,\n",
        "            references=formatted_refs\n",
        "        )\n",
        "\n",
        "        return result[\"bleu\"] * 100\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing BLEU with evaluate library: {e}\")\n",
        "        # Fallback to manual implementation\n",
        "        return compute_bleu_score_manual(references, predictions)\n",
        "\n",
        "def compute_bleu_score_manual(references, predictions):\n",
        "    \"\"\"\n",
        "    Manual BLEU implementation as fallback\n",
        "    Compatible with all Python versions\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    import math\n",
        "\n",
        "    def get_ngrams(tokens, n):\n",
        "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "    def compute_bleu_for_sentence(reference, prediction):\n",
        "        ref_tokens = reference.split()\n",
        "        pred_tokens = prediction.split()\n",
        "\n",
        "        if len(pred_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Compute precision for n-grams (1 to 4)\n",
        "        precisions = []\n",
        "        for n in range(1, 5):\n",
        "            if len(pred_tokens) < n:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            pred_ngrams = get_ngrams(pred_tokens, n)\n",
        "            ref_ngrams = get_ngrams(ref_tokens, n)\n",
        "\n",
        "            if len(pred_ngrams) == 0:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            pred_counter = Counter(pred_ngrams)\n",
        "            ref_counter = Counter(ref_ngrams)\n",
        "\n",
        "            overlap = 0\n",
        "            for ngram in pred_counter:\n",
        "                overlap += min(pred_counter[ngram], ref_counter[ngram])\n",
        "\n",
        "            precision = overlap / len(pred_ngrams) if len(pred_ngrams) > 0 else 0.0\n",
        "            precisions.append(precision)\n",
        "\n",
        "        # Brevity penalty\n",
        "        if len(pred_tokens) > 0:\n",
        "            bp = min(1.0, math.exp(1 - len(ref_tokens) / len(pred_tokens)))\n",
        "        else:\n",
        "            bp = 0.0\n",
        "\n",
        "        # Geometric mean of precisions\n",
        "        if all(p > 0 for p in precisions):\n",
        "            bleu = bp * math.exp(sum(math.log(p) for p in precisions) / 4)\n",
        "        else:\n",
        "            bleu = 0.0\n",
        "\n",
        "        return bleu\n",
        "\n",
        "    scores = []\n",
        "    for ref_list, pred in zip(references, predictions):\n",
        "        ref = ref_list[0] if isinstance(ref_list, list) else ref_list\n",
        "        score = compute_bleu_for_sentence(ref, pred)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores) * 100 if scores else 0.0\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    global current_best_bleu\n",
        "\n",
        "    start_time = time.time()  # Start time of trial\n",
        "\n",
        "\n",
        "    # Hyperparameter suggestions - only 3 parameters\n",
        "    # learning_rate = trial.suggest_categorical('learning_rate', [2e-5, 3e-5, 4e-5, 5e-5])\n",
        "    # batch_size = trial.suggest_categorical('batch_size', [4, 6, 8, 12])\n",
        "    # weight_decay = trial.suggest_categorical('weight_decay', [1e-6, 1e-5, 5e-5, 1e-4])\n",
        "\n",
        "    # jika gridsearch\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', search_space_optimal['learning_rate'])\n",
        "    batch_size = trial.suggest_categorical('batch_size', search_space_optimal['batch_size'])\n",
        "    weight_decay = trial.suggest_categorical('weight_decay', search_space_optimal['weight_decay'])\n",
        "\n",
        "    # Log trial parameters\n",
        "    print(f\"\\n=== Trial {trial.number} ===\")\n",
        "    print(f\"LR: {learning_rate}, Batch: {batch_size}, WD: {weight_decay}\")\n",
        "    print(f\"Current Best BLEU: {current_best_bleu:.4f}%\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model setup - standard T5 without dropout modification\n",
        "    MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # Optimizer setup\n",
        "    OPTIMIZER = AdamW(MODEL.parameters(), lr=learning_rate, eps=1e-8, weight_decay=weight_decay)\n",
        "\n",
        "    # Data loaders (assuming train_dataset and val_dataset are defined globally)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_bleu_this_trial = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    trial_status = \"Ongoing\"\n",
        "    trial_data = []\n",
        "\n",
        "    # Early stopping for Val Loss\n",
        "    val_loss_patience_counter  = 0\n",
        "    val_loss_patience = 2\n",
        "\n",
        "    # Training and validation loop\n",
        "    for epoch in range(10):  # 10 epochs as in original\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_batch_count = 0\n",
        "        val_batch_count = 0\n",
        "\n",
        "        # TRAINING\n",
        "        MODEL.train()\n",
        "        for batch in tqdm(train_loader, desc=f'[Trial {trial.number}] Epoch {epoch+1} - Training'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = MODEL(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=decoder_attention_mask\n",
        "            )\n",
        "\n",
        "            # Optimization\n",
        "            OPTIMIZER.zero_grad()\n",
        "            outputs.loss.backward()\n",
        "            OPTIMIZER.step()\n",
        "\n",
        "            train_loss += outputs.loss.item()\n",
        "            train_batch_count += 1\n",
        "\n",
        "        # VALIDATION\n",
        "        MODEL.eval()\n",
        "        for batch in tqdm(val_loader, desc=f'[Trial {trial.number}] Epoch {epoch+1} - Validation'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = MODEL(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,\n",
        "                    decoder_attention_mask=decoder_attention_mask\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "            val_batch_count += 1\n",
        "\n",
        "        # Calculate losses\n",
        "        avg_train_loss = train_loss / train_batch_count\n",
        "        avg_val_loss = val_loss / val_batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # COMPUTE BLEU THIS EPOCH\n",
        "        val_refs_bleu, val_hyps_bleu = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['source_ids'].to(device)\n",
        "                attention_mask = batch['source_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                preds = MODEL.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_length=target_max_len  # Assuming this is defined globally\n",
        "                )\n",
        "\n",
        "                decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in preds]\n",
        "                decoded_labels = [tokenizer.decode([t for t in label if t != -100], skip_special_tokens=True) for label in labels]\n",
        "\n",
        "                val_hyps_bleu.extend(decoded_preds)\n",
        "                val_refs_bleu.extend([[ref] for ref in decoded_labels])\n",
        "\n",
        "        # Calculate BLEU using sacreBLEU\n",
        "        flattened_refs = [ref[0] if isinstance(ref, list) else ref for ref in val_refs_bleu]\n",
        "        epoch_bleu = compute_bleu_score_sacrebleu(flattened_refs, val_hyps_bleu)\n",
        "\n",
        "        # Alternative: Use Hugging Face BLEU\n",
        "        # epoch_bleu = compute_bleu_score_identical_to_baseline(val_refs_bleu, val_hyps_bleu)\n",
        "\n",
        "        # Update BLEU tracking\n",
        "        if epoch_bleu > best_bleu_this_trial:\n",
        "            best_bleu_this_trial = epoch_bleu\n",
        "\n",
        "        # Update global best BLEU\n",
        "        if current_best_bleu < best_bleu_this_trial:\n",
        "            current_best_bleu = best_bleu_this_trial\n",
        "\n",
        "        print(f\"[Trial {trial.number}] Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f} | BLEU = {epoch_bleu:.2f}% | Current Best BLEU = {current_best_bleu:.2f}%\")\n",
        "\n",
        "        # Store trial data for CSV logging\n",
        "        trial_data.append({\n",
        "            'Trial': trial.number,\n",
        "            'Learning Rate': learning_rate,\n",
        "            'Batch Size': batch_size,\n",
        "            'Weight Decay': weight_decay,\n",
        "            'Epoch': epoch + 1,\n",
        "            'Train Loss': avg_train_loss,\n",
        "            'Val Loss': avg_val_loss,\n",
        "            'BLEU': epoch_bleu,\n",
        "            'Status': trial_status,\n",
        "            'GPU Memory (GB)': torch.cuda.max_memory_allocated()/1024**3 if torch.cuda.is_available() else 0\n",
        "        })\n",
        "\n",
        "        # PRUNING CHECK (disable when Gridsearch)\n",
        "        # trial.report(-epoch_bleu, epoch)\n",
        "        # if trial.should_prune():\n",
        "        #     print(f\"[Trial {trial.number}] PRUNED at epoch {epoch+1} based on BLEU\")\n",
        "        #     trial_status = \"Pruned\"\n",
        "        #     trial.set_user_attr(\"status\", trial_status)\n",
        "        #     trial.set_user_attr(\"final_train_loss\", f\"{avg_train_loss:.6f}\")\n",
        "        #     trial.set_user_attr(\"final_val_loss\", f\"{avg_val_loss:.6f}\")\n",
        "        #     trial.set_user_attr(\"epochs_completed\", epoch + 1)\n",
        "        #     trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "        #     end_time = time.time()\n",
        "        #     duration = end_time - start_time\n",
        "        #     formatted_duration = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
        "        #     print(f\"[Trial {trial.number}] Duration: {formatted_duration} ({duration:.2f} seconds)\")\n",
        "\n",
        "        #     cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "        #     raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        # EARLY STOPPING based on validation loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            val_loss_patience_counter  = 0\n",
        "        else:\n",
        "            val_loss_patience_counter  += 1\n",
        "            print(f\"Val Loss Patience Counter: {val_loss_patience_counter} / {val_loss_patience}\")\n",
        "            if val_loss_patience_counter > val_loss_patience:\n",
        "                print(f\"[Trial {trial.number}] EARLY STOPPED at epoch {epoch+1}\")\n",
        "                trial_status = \"Early Stopped\"\n",
        "                trial.set_user_attr(\"status\", trial_status)\n",
        "                trial.set_user_attr(\"final_train_loss\", f\"{avg_train_loss:.6f}\")\n",
        "                trial.set_user_attr(\"final_val_loss\", f\"{avg_val_loss:.6f}\")\n",
        "                trial.set_user_attr(\"epochs_completed\", epoch + 1)\n",
        "                trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "                end_time = time.time()\n",
        "                duration = end_time - start_time\n",
        "                formatted_duration = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
        "                print(f\"[Trial {trial.number}] Duration: {formatted_duration} ({duration:.2f} seconds)\")\n",
        "\n",
        "                cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "                break\n",
        "\n",
        "    print(f\"[Trial {trial.number}] Final BLEU-4: {best_bleu_this_trial:.4f}\")\n",
        "    print(f\"[Trial {trial.number}] Final Train Loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"[Trial {trial.number}] Final Val Loss: {val_losses[-1]:.6f}\")\n",
        "\n",
        "    # Store final trial results\n",
        "    if trial_status == \"Ongoing\":\n",
        "        trial_status = \"Completed\"\n",
        "\n",
        "    trial.set_user_attr(\"status\", trial_status)\n",
        "    trial.set_user_attr(\"final_train_loss\", f\"{train_losses[-1]:.6f}\")\n",
        "    trial.set_user_attr(\"final_val_loss\", f\"{val_losses[-1]:.6f}\")\n",
        "    trial.set_user_attr(\"epochs_completed\", len(train_losses))\n",
        "    trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "    # Save trial data to CSV (assuming OPTUNA_PATH is defined)\n",
        "    if 'OPTUNA_PATH' in globals():\n",
        "        trial_df = pd.DataFrame(trial_data)\n",
        "        csv_file = os.path.join(OPTUNA_PATH, f'trial_{trial.number}_results.csv')\n",
        "        trial_df.to_csv(csv_file, index=False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    formatted_duration = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
        "    print(f\"[Trial {trial.number}] Duration: {formatted_duration} ({duration:.2f} seconds)\")\n",
        "\n",
        "    cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "\n",
        "    return -best_bleu_this_trial  # Negative for minimization\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "print(\"=\"*60)\n",
        "print(\"HPO STUDY STARTS NOW!!!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Create Optuna study\n",
        "\n",
        "# Metode Gridsearch (brute force semua kombinasi)\n",
        "# search_space = {\n",
        "#     \"learning_rate\": [2e-5, 3e-5, 4e-5, 5e-5],\n",
        "#     \"batch_size\": [4, 6, 8, 12],\n",
        "#     \"weight_decay\": [1e-6, 1e-5, 5e-5, 1e-4]\n",
        "# }\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=optuna.samplers.GridSampler(search_space)\n",
        "# )\n",
        "\n",
        "# OPTIMAL NARROW GRIDSEARCH CONFIGURATION\n",
        "# Total combinations: 4×3×3 = 36\n",
        "# 100% Coverage\n",
        "search_space_optimal = {\n",
        "    \"learning_rate\": [2.8e-5, 3e-5, 3.2e-5, 3.5e-5],   # Fine-grain around best\n",
        "    \"batch_size\": [6, 7, 8],                            # Only proven winners\n",
        "    \"weight_decay\": [9e-5, 1e-4, 1.1e-4]               # Fine-tune around dominant\n",
        "}\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    sampler=optuna.samplers.GridSampler(search_space_optimal),\n",
        "    study_name=\"QG_T5_GridSearch_Optimal\"\n",
        ")\n",
        "study.optimize(objective, n_trials=36)  # 36 = total combinations (4×3×3)\n",
        "\n",
        "# study untuk menggunakan TPE sampler custom config\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     study_name=\"QG_T5_HPO_CLEAN\",\n",
        "#     sampler=optuna.samplers.TPESampler(\n",
        "#         n_startup_trials=3,    # Reduced from default 10\n",
        "#         n_ei_candidates=12     # Focus search efficiency\n",
        "#     ),\n",
        "#     pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
        "# )\n",
        "\n",
        "# study 64 kombinasi TPE sampler custom config\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     study_name=\"QG_T5_TPE_64Search\",\n",
        "#     sampler=optuna.samplers.TPESampler(\n",
        "#         # random trials awal sebelum TPE mulai membangun distribusi probabilistik dari parameter yang bagus dan buruk.\n",
        "#         n_startup_trials=8,       # 12.5% dari 64\n",
        "#         # menentukan berapa kandidat yang akan dipertimbangkan dalam optimasi Expected Improvement (EI).\n",
        "#         n_ei_candidates=16        # cek 25% dari kombinasi, cukup agresif\n",
        "#     ),\n",
        "#     pruner=MedianPruner(n_startup_trials=8, n_warmup_steps=2)\n",
        "# )\n",
        "# study.optimize(objective, n_trials=32)  # ~50% coverage\n",
        "\n",
        "# study untuk menggunakan default konfigurasi TPE (n_startup = 10, n_ei_cand = 24)\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     study_name=\"QG_T5_HPO_WITHOUT_DROPOUT\",\n",
        "#     pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
        "# )\n",
        "\n",
        "\n",
        "# # Run optimization\n",
        "# try:\n",
        "#     study.optimize(objective, n_trials=20)\n",
        "# except Exception as e:\n",
        "#     print(f\"Optuna optimization failed: {e}\")\n",
        "#     print(\"Continuing with available results...\")\n",
        "\n",
        "# RESULTS ANALYSIS\n",
        "if study.trials:\n",
        "    best_trial = study.best_trial\n",
        "    best_bleu = -best_trial.value\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"HPO STUDY RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best BLEU-4 Score: {best_bleu:.4f}\")\n",
        "\n",
        "    print(f\"\\nBest Hyperparameters:\")\n",
        "    for param in best_trial.params:\n",
        "        print(f\"• {param.replace('_', ' ').title()}: {best_trial.params[param]}\")\n",
        "\n",
        "    # Analyze all trials\n",
        "    # successful_trials = [t for t in study.trials if t.value != float('inf')]\n",
        "    successful_trials = [t for t in study.trials if t.value is not None and t.value != float('inf')]\n",
        "    print(f\"\\nAll Trial Results:\")\n",
        "    print(\"Rank | BLEU-4  | LR     | BS | WD     | Status\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Sort by BLEU score\n",
        "    trial_results = [(t, -t.value) for t in successful_trials]\n",
        "    trial_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (trial, bleu) in enumerate(trial_results[:10], 1):  # Top 10\n",
        "        lr = trial.params['learning_rate']\n",
        "        bs = trial.params['batch_size']\n",
        "        wd = trial.params['weight_decay']\n",
        "        status = trial.user_attrs.get('status', 'Unknown')[:8]\n",
        "        print(f\"{i:4d} | {bleu:6.2f}% | {lr:.0e} | {bs:2d} | {wd:.0e} | {status}\")\n",
        "\n",
        "    # Parameter effectiveness analysis\n",
        "    print(f\"\\nParameter Effectiveness Analysis:\")\n",
        "\n",
        "    # Group by parameter values\n",
        "    from collections import defaultdict\n",
        "\n",
        "    param_stats = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for trial, bleu in trial_results:\n",
        "        for param in ['learning_rate', 'batch_size', 'weight_decay']:\n",
        "            value = trial.params[param]\n",
        "            param_stats[param][value].append(bleu)\n",
        "\n",
        "    print(\"\\nAverage BLEU-4 by parameter value:\")\n",
        "\n",
        "    for param in ['learning_rate', 'batch_size', 'weight_decay']:\n",
        "        print(f\"\\n{param.replace('_', ' ').title()}:\")\n",
        "        for val in sorted(param_stats[param].keys()):\n",
        "            avg_bleu = np.mean(param_stats[param][val])\n",
        "            n_trials = len(param_stats[param][val])\n",
        "            if isinstance(val, float):\n",
        "                print(f\"  {val:.2e}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "            else:\n",
        "                print(f\"  {val:>7}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "\n",
        "    # Save detailed results\n",
        "    results_data = []\n",
        "    for trial, bleu in trial_results:\n",
        "        results_data.append({\n",
        "            'Trial': trial.number,\n",
        "            'BLEU_4': bleu,\n",
        "            'Learning_Rate': trial.params['learning_rate'],\n",
        "            'Batch_Size': trial.params['batch_size'],\n",
        "            'Weight_Decay': trial.params['weight_decay'],\n",
        "            'Status': trial.user_attrs.get('status', 'Unknown'),\n",
        "            'Final_Train_Loss': trial.user_attrs.get('final_train_loss', 0),\n",
        "            'Final_Val_Loss': trial.user_attrs.get('final_val_loss', 0),\n",
        "            'Epochs_Completed': trial.user_attrs.get('epochs_completed', 0)\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "\n",
        "    # Save results (assuming LOG_PATH is defined)\n",
        "    if 'LOG_PATH' in globals():\n",
        "        csv_path = os.path.join(LOG_PATH, f'hpo_study_postAnalysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
        "        results_df.to_csv(csv_path, index=False)\n",
        "        print(f\"\\nDetailed results saved to: {csv_path}\")\n",
        "\n",
        "    print(f\"\\nSUMMARY:\")\n",
        "    print(f\"• {len(successful_trials)}/{len(study.trials)} trials successful\")\n",
        "    print(f\"• Best BLEU-4: {best_bleu:.4f}%\")\n",
        "    print(f\"• Best hyperparameters:\")\n",
        "    for param, value in best_trial.params.items():\n",
        "        print(f\"  - {param}: {value}\")\n",
        "\n",
        "    # Trial status breakdown\n",
        "    status_counts = defaultdict(int)\n",
        "    for trial in study.trials:\n",
        "        status = trial.user_attrs.get('status', 'Unknown')\n",
        "        status_counts[status] += 1\n",
        "\n",
        "    print(f\"\\nTrial Status Breakdown:\")\n",
        "    for status, count in status_counts.items():\n",
        "        print(f\"• {status}: {count} trials\")\n",
        "\n",
        "else:\n",
        "    print(\"No trials completed successfully.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HPO STUDY COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFY3WLx83K6C",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## HPO NYA dengan Dropout Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm2hTSSQGcyg"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Optimazion Optuna\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
        "from optuna.pruners import MedianPruner\n",
        "from evaluate import load\n",
        "import sacrebleu\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "def auto_scroll_output():\n",
        "    display(Javascript('''\n",
        "        var outCells = document.querySelectorAll('.output_scroll');\n",
        "        if (outCells.length > 0) {\n",
        "            var lastOut = outCells[outCells.length - 1];\n",
        "            lastOut.scrollTop = lastOut.scrollHeight;\n",
        "        }\n",
        "    '''))\n",
        "\n",
        "# Global variable to track best BLEU score\n",
        "current_best_bleu = 0.0\n",
        "\n",
        "def cleanup_memory(*objects):\n",
        "    for obj in objects:\n",
        "        del obj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def compute_bleu_score_sacrebleu(references, predictions):\n",
        "    \"\"\"\n",
        "    Compute BLEU score using sacreBLEU (corpus-level)\n",
        "    References should be a list of strings\n",
        "    Predictions should be a list of strings\n",
        "    \"\"\"\n",
        "    return sacrebleu.corpus_bleu(predictions, [references]).score\n",
        "\n",
        "def compute_bleu_score_identical_to_baseline(references, predictions):\n",
        "    \"\"\"\n",
        "    Compute BLEU score using Hugging Face evaluate library\n",
        "    This is more stable and compatible with newer Python versions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load BLEU metric from Hugging Face evaluate\n",
        "        bleu_metric = load(\"bleu\")\n",
        "\n",
        "        # Format references as list of lists (required by evaluate library)\n",
        "        formatted_refs = [[ref] for ref in references]\n",
        "\n",
        "        # Compute BLEU score\n",
        "        result = bleu_metric.compute(\n",
        "            predictions=predictions,\n",
        "            references=formatted_refs\n",
        "        )\n",
        "\n",
        "        return result[\"bleu\"] * 100\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing BLEU with evaluate library: {e}\")\n",
        "        # Fallback to manual implementation\n",
        "        return compute_bleu_score_manual(references, predictions)\n",
        "\n",
        "def compute_bleu_score_manual(references, predictions):\n",
        "    \"\"\"\n",
        "    Manual BLEU implementation as fallback\n",
        "    Compatible with all Python versions\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    import math\n",
        "\n",
        "    def get_ngrams(tokens, n):\n",
        "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "    def compute_bleu_for_sentence(reference, prediction):\n",
        "        ref_tokens = reference.split()\n",
        "        pred_tokens = prediction.split()\n",
        "\n",
        "        if len(pred_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Compute precision for n-grams (1 to 4)\n",
        "        precisions = []\n",
        "        for n in range(1, 5):\n",
        "            if len(pred_tokens) < n:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            pred_ngrams = get_ngrams(pred_tokens, n)\n",
        "            ref_ngrams = get_ngrams(ref_tokens, n)\n",
        "\n",
        "            if len(pred_ngrams) == 0:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            pred_counter = Counter(pred_ngrams)\n",
        "            ref_counter = Counter(ref_ngrams)\n",
        "\n",
        "            overlap = 0\n",
        "            for ngram in pred_counter:\n",
        "                overlap += min(pred_counter[ngram], ref_counter[ngram])\n",
        "\n",
        "            precision = overlap / len(pred_ngrams) if len(pred_ngrams) > 0 else 0.0\n",
        "            precisions.append(precision)\n",
        "\n",
        "        # Brevity penalty\n",
        "        if len(pred_tokens) > 0:\n",
        "            bp = min(1.0, math.exp(1 - len(ref_tokens) / len(pred_tokens)))\n",
        "        else:\n",
        "            bp = 0.0\n",
        "\n",
        "        # Geometric mean of precisions\n",
        "        if all(p > 0 for p in precisions):\n",
        "            bleu = bp * math.exp(sum(math.log(p) for p in precisions) / 4)\n",
        "        else:\n",
        "            bleu = 0.0\n",
        "\n",
        "        return bleu\n",
        "\n",
        "    scores = []\n",
        "    for ref_list, pred in zip(references, predictions):\n",
        "        ref = ref_list[0] if isinstance(ref_list, list) else ref_list\n",
        "        score = compute_bleu_for_sentence(ref, pred)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores) * 100 if scores else 0.0\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    global current_best_bleu\n",
        "\n",
        "    start_time = time.time()  # Start time of trial\n",
        "\n",
        "    # # Search space HPO with Dropout rate (TPE)\n",
        "    # learning_rate = trial.suggest_categorical('learning_rate', [3e-5, 4e-5, 5e-5])\n",
        "    # batch_size = trial.suggest_categorical('batch_size', [6, 8, 12])\n",
        "    # weight_decay = trial.suggest_categorical('weight_decay', [1e-5, 5e-5, 1e-4])\n",
        "    # dropout_rate = trial.suggest_categorical('dropout_rate', [0.1, 0.15, 0.2])\n",
        "\n",
        "    # # Search space HPO with Dropout rate (Gridsearch)\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', search_space_conservative['learning_rate'])\n",
        "    batch_size = trial.suggest_categorical('batch_size', search_space_conservative['batch_size'])\n",
        "    weight_decay = trial.suggest_categorical('weight_decay', search_space_conservative['weight_decay'])\n",
        "    dropout_rate = trial.suggest_categorical('dropout_rate', search_space_conservative['dropout_rate'])\n",
        "\n",
        "    #  # 1. DROPOUT RATE - Narrow ke optimal zone\n",
        "    # dropout_rate = trial.suggest_float('dropout_rate', 0.10, 0.15)\n",
        "\n",
        "    # # 2. LEARNING RATE - High precision search di sweet spot\n",
        "    # learning_rate = trial.suggest_float('learning_rate', 1.25e-05, 1.75e-05)\n",
        "\n",
        "    # # 3. BATCH SIZE - Fokus pada yang terbukti bagus\n",
        "    # batch_size = trial.suggest_categorical('batch_size', [4, 6])\n",
        "\n",
        "    # # 4. WEIGHT DECAY - Fokus pada range rendah\n",
        "    # weight_decay = trial.suggest_float('weight_decay', 5e-07, 1e-05, log=True)\n",
        "\n",
        "    # Log trial parameters\n",
        "    print(f\"\\n=== Trial {trial.number} ===\")\n",
        "    print(f\"LR: {learning_rate}, Batch: {batch_size}, WD: {weight_decay}, Dropout: {dropout_rate}\")\n",
        "    # print(f\"LR: {learning_rate}, Batch: {batch_size}, WD: {weight_decay}\")\n",
        "    print(f\"Current Best BLEU: {current_best_bleu:.4f}%\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    config = T5Config.from_pretrained(\"t5-base\")\n",
        "    config.dropout_rate = dropout_rate\n",
        "    MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", config=config).to(device)\n",
        "    # MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # IDENTICAL optimizer setup to original - only LR and weight_decay change\n",
        "    OPTIMIZER = AdamW(MODEL.parameters(), lr=learning_rate, eps=1e-8, weight_decay=weight_decay)\n",
        "\n",
        "    # IDENTICAL data loaders to original - only batch_size changes\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_bleu_this_trial = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    trial_status = \"On going\"  # Default\n",
        "    trial_data = []\n",
        "\n",
        "    # Early stopping for BLEU\n",
        "    val_loss_patience_counter = 0\n",
        "    val_loss_patience = 3\n",
        "\n",
        "    # training and val loop\n",
        "    for epoch in range(20):  # 10 to 20 karena menggunakan dropout\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_batch_count = 0\n",
        "        val_batch_count = 0\n",
        "\n",
        "        # TRAINING - identical to original\n",
        "        MODEL.train()\n",
        "        for batch in tqdm(train_loader, desc=f'[Trial {trial.number}] Epoch {epoch+1} - Training'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = MODEL(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=decoder_attention_mask\n",
        "            )\n",
        "\n",
        "            # Optimization\n",
        "            OPTIMIZER.zero_grad()\n",
        "            outputs.loss.backward()\n",
        "            OPTIMIZER.step()\n",
        "\n",
        "            train_loss += outputs.loss.item()\n",
        "            train_batch_count += 1\n",
        "\n",
        "        # VALIDATION\n",
        "        MODEL.eval()\n",
        "        for batch in tqdm(val_loader, desc=f'[Trial {trial.number}] Epoch {epoch+1} - Validation'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = MODEL(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,\n",
        "                    decoder_attention_mask=decoder_attention_mask\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "            val_batch_count += 1\n",
        "\n",
        "        # Calculate losses\n",
        "        avg_train_loss = train_loss / train_batch_count\n",
        "        avg_val_loss = val_loss / val_batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # COMPUTE BLEU THIS EPOCH\n",
        "        val_refs_bleu, val_hyps_bleu = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['source_ids'].to(device)\n",
        "                attention_mask = batch['source_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                preds = MODEL.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_length=target_max_len\n",
        "                )\n",
        "\n",
        "                decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in preds]\n",
        "                decoded_labels = [tokenizer.decode([t for t in label if t != -100], skip_special_tokens=True) for label in labels]\n",
        "\n",
        "                val_hyps_bleu.extend(decoded_preds)\n",
        "                val_refs_bleu.extend([[ref] for ref in decoded_labels])\n",
        "\n",
        "        # sacreBLEU\n",
        "        flattened_refs = [ref[0] if isinstance(ref, list) else ref for ref in val_refs_bleu]\n",
        "        epoch_bleu = compute_bleu_score_sacrebleu(flattened_refs, val_hyps_bleu)\n",
        "\n",
        "        #  Hugging Face NLTK BLEU\n",
        "        # epoch_bleu = compute_bleu_score_identical_to_baseline(val_refs_bleu, val_hyps_bleu)\n",
        "\n",
        "        # Update BLEU tracking\n",
        "        if epoch_bleu > best_bleu_this_trial:\n",
        "            best_bleu_this_trial = epoch_bleu\n",
        "\n",
        "        if current_best_bleu < best_bleu_this_trial:\n",
        "            current_best_bleu = best_bleu_this_trial\n",
        "\n",
        "        print(f\"[Trial {trial.number}] Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f} | BLEU = {epoch_bleu:.2f}% | Current Best BLEU = {current_best_bleu:.2f}%\")\n",
        "\n",
        "        auto_scroll_output() # autoscroll output cell notebook\n",
        "\n",
        "        # Store trial data to CSV\n",
        "        trial_data.append({\n",
        "            'Trial': trial.number,\n",
        "            'Learning Rate': learning_rate,\n",
        "            'Batch Size': batch_size,\n",
        "            'Weight Decay': weight_decay,\n",
        "            'Dropout Rate': dropout_rate,\n",
        "            'Epoch': epoch + 1,\n",
        "            'Train Loss': avg_train_loss,\n",
        "            'Val Loss': avg_val_loss,\n",
        "            'BLEU': epoch_bleu,\n",
        "            'Status': trial_status,\n",
        "            'GPU Memory (GB)': torch.cuda.max_memory_allocated()/1024**3\n",
        "        })\n",
        "\n",
        "        # PRUNING\n",
        "        # trial.report(-epoch_bleu, epoch)\n",
        "        # if trial.should_prune():\n",
        "        #     print(f\"[Trial {trial.number}] PRUNED at epoch {epoch+1} based on BLEU\")\n",
        "        #     trial_status = \"Pruned\"\n",
        "        #     trial.set_user_attr(\"status\", trial_status)\n",
        "        #     trial.set_user_attr(\"final_train_loss\", f\"{avg_train_loss:.6f}\")\n",
        "        #     trial.set_user_attr(\"final_val_loss\", f\"{avg_val_loss:.6f}\")\n",
        "        #     trial.set_user_attr(\"epochs_completed\", epoch + 1)\n",
        "        #     trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "        #     end_time = time.time()\n",
        "        #     duration = end_time - start_time\n",
        "        #     formatted_duration = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
        "        #     print(f\"[Trial {trial.number}] Duration: {formatted_duration} ({duration:.2f} seconds)\")\n",
        "\n",
        "        #     cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "        #     raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        # EARLY STOPPING\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            val_loss_patience_counter = 0\n",
        "        else:\n",
        "            val_loss_patience_counter += 1\n",
        "            print(f\"Val Loss Patience Counter: {val_loss_patience_counter} / {val_loss_patience}\")\n",
        "            if val_loss_patience_counter > val_loss_patience:\n",
        "                print(f\"[Trial {trial.number}] EARLY STOPPED at epoch {epoch+1}\")\n",
        "                trial_status = \"Early Stopped\"\n",
        "                trial.set_user_attr(\"status\", trial_status)\n",
        "                trial.set_user_attr(\"final_train_loss\", f\"{avg_train_loss:.6f}\")\n",
        "                trial.set_user_attr(\"final_val_loss\", f\"{avg_val_loss:.6f}\")\n",
        "                trial.set_user_attr(\"epochs_completed\", epoch + 1)\n",
        "                trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "                cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "                break\n",
        "\n",
        "\n",
        "    print(f\"[Trial {trial.number}] Final BLEU-4: {best_bleu_this_trial:.4f}\")\n",
        "    print(f\"[Trial {trial.number}] Final Train Loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"[Trial {trial.number}] Final Val Loss: {val_losses[-1]:.6f}\")\n",
        "\n",
        "    # Store results for analysis\n",
        "    if trial_status == \"Ongoing\":\n",
        "        trial_status = \"Completed\"\n",
        "\n",
        "    trial.set_user_attr(\"status\", trial_status)\n",
        "    trial.set_user_attr(\"final_train_loss\", f\"{train_losses[-1]:.6f}\")  # FIXED\n",
        "    trial.set_user_attr(\"final_val_loss\", f\"{val_losses[-1]:.6f}\")      # FIXED\n",
        "    trial.set_user_attr(\"epochs_completed\", len(train_losses))\n",
        "    trial.set_user_attr(\"best_bleu_epoch\", best_bleu_this_trial)\n",
        "\n",
        "    # Save trial data to CSV\n",
        "    trial_df = pd.DataFrame(trial_data)\n",
        "    csv_file = os.path.join(OPTUNA_PATH, f'trial_{trial.number}_results.csv')\n",
        "    if os.path.exists(csv_file):\n",
        "        trial_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        trial_df.to_csv(csv_file, index=False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    formatted_duration = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
        "    print(f\"[Trial {trial.number}] Duration: {formatted_duration} ({duration:.2f} seconds)\")\n",
        "\n",
        "    cleanup_memory(MODEL, OPTIMIZER, train_loader, val_loader)\n",
        "\n",
        "    return -best_bleu_this_trial  # Negative for minimization\n",
        "\n",
        "# MAIN EXECUTION\n",
        "print(\"=\"*60)\n",
        "print(\"HPO STUDY starts now!!!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# TPE\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     study_name=\"QG_T5_TPE_WithDropout\",\n",
        "#     sampler=optuna.samplers.TPESampler(\n",
        "#         # random trials awal sebelum TPE mulai membangun distribusi probabilistik dari parameter yang bagus dan buruk.\n",
        "#         n_startup_trials=12,      # 15% dari 81 combinations\n",
        "#         # menentukan berapa kandidat yang akan dipertimbangkan dalam optimasi Expected Improvement (EI).\n",
        "#         n_ei_candidates=20        # ~25% dari 81 combinations, balanced exploration\n",
        "#     ),\n",
        "#     pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
        "# )\n",
        "\n",
        "# Conservative GridSearch berdasarkan proven high-performance zone\n",
        "search_space_conservative = {\n",
        "    \"learning_rate\": [4.5e-5, 5e-5, 5.5e-5],        # Tight around LR=5e-05 winner\n",
        "    \"batch_size\": [6, 7, 8],                         # Focus pada proven performers\n",
        "    \"weight_decay\": [1e-5, 2e-5],                    # Lower WD preferred with dropout\n",
        "    \"dropout_rate\": [0.10, 0.125, 0.15]             # Optimal dropout range only\n",
        "}\n",
        "\n",
        "# GridSearch Study Configuration\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"QG_T5_GridSearch_DropoutOptimal\",\n",
        "    sampler=optuna.samplers.GridSampler(search_space_conservative),\n",
        "    pruner=optuna.pruners.NopPruner()               # No pruning untuk exhaustive search\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=54)              # 3×3×2×3 = 54 combinations\n",
        "\n",
        "# try:\n",
        "#     study.optimize(objective, n_trials=48)  # ~60% coverage\n",
        "# except Exception as e:\n",
        "#     print(f\"Optuna optimization failed: {e}\")\n",
        "#     print(\"Continuing with available results...\")\n",
        "\n",
        "# RESULTS ANALYSIS\n",
        "if study.trials:\n",
        "    best_trial = study.best_trial\n",
        "    best_bleu = -best_trial.value\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"HPO STUDY RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best BLEU-4 Score: {best_bleu:.4f}\")\n",
        "\n",
        "    # print(f\"\\nBest Hyperparameters:\")\n",
        "    # print(f\"• Learning Rate: {best_trial.params['learning_rate']}\")\n",
        "    # print(f\"• Batch Size: {best_trial.params['batch_size']}\")\n",
        "    # print(f\"• Weight Decay: {best_trial.params['weight_decay']}\")\n",
        "    print(f\"\\nBest Hyperparameters:\")\n",
        "    for param in best_trial.params:\n",
        "        print(f\"• {param.replace('_', ' ').title()}: {best_trial.params[param]}\")\n",
        "\n",
        "    # Analyze all trials\n",
        "    # successful_trials = [t for t in study.trials if t.value != float('inf')]\n",
        "    successful_trials = [t for t in study.trials if t.value is not None and t.value != float('inf')]\n",
        "    print(f\"\\nAll Trial Results:\")\n",
        "    print(\"Rank | BLEU-4  | LR     | BS | WD     | DO\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    # Sort by BLEU score\n",
        "    trial_results = [(t, -t.value) for t in successful_trials]\n",
        "    trial_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (trial, bleu) in enumerate(trial_results[:10], 1):  # Top 10\n",
        "        lr = trial.params['learning_rate']\n",
        "        bs = trial.params['batch_size']\n",
        "        wd = trial.params['weight_decay']\n",
        "        do = trial.params['dropout_rate']\n",
        "        print(f\"{i:4d} | {bleu:6.2f}% | {lr:.0e} | {bs:2d} | {wd:.0e} | {do:.2f}\")\n",
        "\n",
        "    # Parameter effectiveness analysis\n",
        "    print(f\"\\nParameter Effectiveness Analysis:\")\n",
        "\n",
        "    # Group by parameter values\n",
        "    from collections import defaultdict\n",
        "\n",
        "    param_stats = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for trial, bleu in trial_results:\n",
        "        for param in ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate']:\n",
        "            value = trial.params[param]\n",
        "            param_stats[param][value].append(bleu)\n",
        "\n",
        "    print(\"\\nAverage BLEU-4 by parameter value:\")\n",
        "\n",
        "    for param in ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate']:\n",
        "        print(f\"\\n{param.replace('_', ' ').title()}:\")\n",
        "        for val in sorted(param_stats[param].keys()):\n",
        "            avg_bleu = np.mean(param_stats[param][val])\n",
        "            n_trials = len(param_stats[param][val])\n",
        "            if isinstance(val, float):\n",
        "                print(f\"  {val:.2e}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "            else:\n",
        "                print(f\"  {val:>7}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "\n",
        "    # Save results\n",
        "    results_data = []\n",
        "    for trial, bleu in trial_results:\n",
        "        results_data.append({\n",
        "            'Trial': trial.number,\n",
        "            'BLEU_4': bleu,\n",
        "            'Learning_Rate': trial.params['learning_rate'],\n",
        "            'Batch_Size': trial.params['batch_size'],\n",
        "            'Weight_Decay': trial.params['weight_decay'],\n",
        "            'Dropout_Rate': trial.params['dropout_rate'],\n",
        "            'Status': trial.user_attrs.get('status', 'Unknown'),\n",
        "            'Final_Train_Loss': trial.user_attrs.get('final_train_loss', 0),\n",
        "            'Final_Val_Loss': trial.user_attrs.get('final_val_loss', 0),\n",
        "            'Epochs_Completed': trial.user_attrs.get('epochs_completed', 0)\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    csv_path = os.path.join(LOG_PATH, f'hpo_study_postAnalysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nDetailed results saved to: {csv_path}\")\n",
        "\n",
        "    print(f\"\\nSUMMARY:\")\n",
        "    print(f\"• {len(successful_trials)}/{len(study.trials)} trials successful\")\n",
        "\n",
        "else:\n",
        "    print(\"No trials completed successfully.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HPO STUDY COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARyV_9nvUyFo"
      },
      "outputs": [],
      "source": [
        "# RESULTS ANALYSIS\n",
        "if study.trials:\n",
        "    best_trial = study.best_trial\n",
        "    best_bleu = -best_trial.value\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"HPO STUDY RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best BLEU-4 Score: {best_bleu:.4f}\")\n",
        "\n",
        "    # print(f\"\\nBest Hyperparameters:\")\n",
        "    # print(f\"• Learning Rate: {best_trial.params['learning_rate']}\")\n",
        "    # print(f\"• Batch Size: {best_trial.params['batch_size']}\")\n",
        "    # print(f\"• Weight Decay: {best_trial.params['weight_decay']}\")\n",
        "    print(f\"\\nBest Hyperparameters:\")\n",
        "    for param in best_trial.params:\n",
        "        print(f\"• {param.replace('_', ' ').title()}: {best_trial.params[param]}\")\n",
        "\n",
        "    # Analyze all trials\n",
        "    # successful_trials = [t for t in study.trials if t.value != float('inf')]\n",
        "    successful_trials = [t for t in study.trials if t.value is not None and t.value != float('inf')]\n",
        "    print(f\"\\nAll Trial Results:\")\n",
        "    print(\"Rank | BLEU-4  | LR     | BS | WD     | DO\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    # Sort by BLEU score\n",
        "    trial_results = [(t, -t.value) for t in successful_trials]\n",
        "    trial_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (trial, bleu) in enumerate(trial_results[:10], 1):  # Top 10\n",
        "        lr = trial.params['learning_rate']\n",
        "        bs = trial.params['batch_size']\n",
        "        wd = trial.params['weight_decay']\n",
        "        do = trial.params['dropout_rate']\n",
        "        print(f\"{i:4d} | {bleu:6.2f}% | {lr:.0e} | {bs:2d} | {wd:.0e} | {do:.2f}\")\n",
        "\n",
        "    # Parameter effectiveness analysis\n",
        "    print(f\"\\nParameter Effectiveness Analysis:\")\n",
        "\n",
        "    # Group by parameter values\n",
        "    from collections import defaultdict\n",
        "\n",
        "    param_stats = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for trial, bleu in trial_results:\n",
        "        for param in ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate']:\n",
        "            value = trial.params[param]\n",
        "            param_stats[param][value].append(bleu)\n",
        "\n",
        "    print(\"\\nAverage BLEU-4 by parameter value:\")\n",
        "\n",
        "    for param in ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate']:\n",
        "        print(f\"\\n{param.replace('_', ' ').title()}:\")\n",
        "        for val in sorted(param_stats[param].keys()):\n",
        "            avg_bleu = np.mean(param_stats[param][val])\n",
        "            n_trials = len(param_stats[param][val])\n",
        "            if isinstance(val, float):\n",
        "                print(f\"  {val:.2e}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "            else:\n",
        "                print(f\"  {val:>7}: {avg_bleu:.2f}% ({n_trials} trials)\")\n",
        "\n",
        "    # Save results\n",
        "    results_data = []\n",
        "    for trial, bleu in trial_results:\n",
        "        results_data.append({\n",
        "            'Trial': trial.number,\n",
        "            'BLEU_4': bleu,\n",
        "            'Learning_Rate': trial.params['learning_rate'],\n",
        "            'Batch_Size': trial.params['batch_size'],\n",
        "            'Weight_Decay': trial.params['weight_decay'],\n",
        "            'Dropout_Rate': trial.params['dropout_rate'],\n",
        "            'Status': trial.user_attrs.get('status', 'Unknown'),\n",
        "            'Final_Train_Loss': trial.user_attrs.get('final_train_loss', 0),\n",
        "            'Final_Val_Loss': trial.user_attrs.get('final_val_loss', 0),\n",
        "            'Epochs_Completed': trial.user_attrs.get('epochs_completed', 0)\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    csv_path = os.path.join(LOG_PATH, f'hpo_study_postAnalysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nDetailed results saved to: {csv_path}\")\n",
        "\n",
        "    print(f\"\\nSUMMARY:\")\n",
        "    print(f\"• {len(successful_trials)}/{len(study.trials)} trials successful\")\n",
        "\n",
        "else:\n",
        "    print(\"No trials completed successfully.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HPO STUDY COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2826B6q3YPY",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Run Analisis Data HPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usU_wbqu3ckv"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING POST-HPO ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Ensure all required variables are available\n",
        "    if 'LOG_PATH' not in globals():\n",
        "        LOG_PATH = './'  # Default to current directory\n",
        "\n",
        "    print(f\"\\nLOG_PATH = {LOG_PATH}\\n\")\n",
        "\n",
        "    # Run complete analysis\n",
        "    print(\"Starting comprehensive HPO analysis...\")\n",
        "    analysis_results = run_complete_hpo_analysis(study, LOG_PATH)\n",
        "\n",
        "    if analysis_results:\n",
        "        print(\"✓ HPO Analysis completed successfully!\")\n",
        "        print(f\"✓ Files saved to: {LOG_PATH}\")\n",
        "    else:\n",
        "        print(\"✗ HPO Analysis failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error running HPO analysis: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Try individual functions if complete analysis fails\n",
        "    print(\"\\nTrying individual analysis functions...\")\n",
        "    try:\n",
        "        print(\"1. Exporting basic trials...\")\n",
        "        trials_df = export_trials_to_csv(study)\n",
        "        print(\"✓ Basic trials exported\")\n",
        "\n",
        "        print(\"2. Creating detailed analysis...\")\n",
        "        detailed_df = create_detailed_tpe_analysis(study)\n",
        "        print(\"✓ Detailed analysis created\")\n",
        "\n",
        "        print(\"3. Generating report...\")\n",
        "        report_file = generate_optimization_report(study)\n",
        "        print(\"✓ Report generated\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Individual functions also failed: {e2}\")\n",
        "\n",
        "# Final cleanup\n",
        "print(\"\\nCleaning up...\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"✓ Cleanup completed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HPO PROCESS FULLY COMPLETED\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t9ZJO_L3jgI",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Visualisasi Data HPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5RO-p73mz8"
      },
      "source": [
        "### Tabel Data dari masing2 Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFEmQxzb3sTD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# UTILITY FUNCTIONS - CSV FILE LOADERS\n",
        "\n",
        "def load_latest_csv(base_path, pattern):\n",
        "    \"\"\"Load the most recent CSV file matching the pattern\"\"\"\n",
        "    files = glob.glob(os.path.join(base_path, f\"{pattern}_*.csv\"))\n",
        "    if not files:\n",
        "        print(f\"No files found matching pattern: {pattern}\")\n",
        "        return None\n",
        "\n",
        "    latest_file = max(files, key=os.path.getctime)\n",
        "    print(f\"Loading: {latest_file}\")\n",
        "    return pd.read_csv(latest_file)\n",
        "\n",
        "def load_all_analysis_data(base_path):\n",
        "    \"\"\"Load all analysis CSV files\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # Load basic trials data\n",
        "    data['trials'] = load_latest_csv(base_path, 'optuna_trials')\n",
        "\n",
        "    # Load detailed TPE analysis\n",
        "    data['detailed_tpe'] = load_latest_csv(base_path, 'detailed_tpe_analysis')\n",
        "\n",
        "    # Load parameter importance\n",
        "    data['param_importance'] = load_latest_csv(base_path, 'parameter_importance')\n",
        "\n",
        "    # Load TPE distributions\n",
        "    data['tpe_distributions'] = load_latest_csv(base_path, 'tpe_distributions_analysis')\n",
        "\n",
        "    # Load performance summary\n",
        "    data['performance_summary'] = load_latest_csv(base_path, 'performance_summary')\n",
        "\n",
        "    return data\n",
        "\n",
        "# 1. OPTIMIZATION HISTORY TABLE\n",
        "\n",
        "def create_optimization_history_table(data):\n",
        "    \"\"\"Create table data for optimization history visualization\"\"\"\n",
        "\n",
        "    if data['detailed_tpe'] is not None:\n",
        "        df = data['detailed_tpe'].copy()\n",
        "    else:\n",
        "        df = data['trials'].copy()\n",
        "\n",
        "    # Extract relevant columns\n",
        "    history_table = pd.DataFrame({\n",
        "        'Trial_Number': df['trial_number'] if 'trial_number' in df.columns else df.get('number', range(len(df))),\n",
        "        'BLEU_Score': df['bleu_score'] if 'bleu_score' in df.columns else -df['value'],\n",
        "        'Objective_Value': df['objective_value'] if 'objective_value' in df.columns else df['value'],\n",
        "        'Best_BLEU_So_Far': df['best_bleu_so_far'] if 'best_bleu_so_far' in df.columns else None,\n",
        "        'Is_Improvement': df['is_improvement'] if 'is_improvement' in df.columns else False,\n",
        "        'Trial_State': df['state'] if 'state' in df.columns else 'COMPLETE',\n",
        "        'Duration_Seconds': df['duration_seconds'] if 'duration_seconds' in df.columns else None,\n",
        "        'Datetime_Complete': df['datetime_complete'] if 'datetime_complete' in df.columns else None\n",
        "    })\n",
        "\n",
        "    # Calculate cumulative best if not available\n",
        "    if history_table['Best_BLEU_So_Far'].isna().all():\n",
        "        history_table['Best_BLEU_So_Far'] = history_table['BLEU_Score'].cummax()\n",
        "        history_table['Is_Improvement'] = history_table['BLEU_Score'] == history_table['Best_BLEU_So_Far']\n",
        "\n",
        "    # Add improvement indicators\n",
        "    history_table['Improvement_From_Previous'] = history_table['BLEU_Score'].diff()\n",
        "    history_table['Trials_Since_Best'] = history_table.groupby((history_table['Is_Improvement']).cumsum()).cumcount()\n",
        "\n",
        "    # Sort by trial number\n",
        "    history_table = history_table.sort_values('Trial_Number').reset_index(drop=True)\n",
        "\n",
        "    return history_table\n",
        "\n",
        "# 2. PARAMETER IMPORTANCE TABLE\n",
        "\n",
        "def create_parameter_importance_table(data):\n",
        "    \"\"\"Create table data for parameter importance visualization\"\"\"\n",
        "\n",
        "    if data['param_importance'] is None:\n",
        "        print(\"No parameter importance data available\")\n",
        "        return None\n",
        "\n",
        "    importance_table = data['param_importance'].copy()\n",
        "\n",
        "    # Add additional metrics\n",
        "    importance_table['Importance_Percentage'] = (\n",
        "        importance_table['importance'] / importance_table['importance'].sum() * 100\n",
        "    )\n",
        "\n",
        "    importance_table['Cumulative_Importance'] = importance_table['Importance_Percentage'].cumsum()\n",
        "\n",
        "    importance_table['Importance_Rank'] = range(1, len(importance_table) + 1)\n",
        "\n",
        "    # Categorize importance levels\n",
        "    importance_table['Importance_Category'] = pd.cut(\n",
        "        importance_table['Importance_Percentage'],\n",
        "        bins=[0, 5, 15, 30, 100],\n",
        "        labels=['Low', 'Medium', 'High', 'Critical']\n",
        "    )\n",
        "\n",
        "    return importance_table\n",
        "\n",
        "# 3. PARALLEL COORDINATE TABLE\n",
        "\n",
        "def create_parallel_coordinate_table(data):\n",
        "    \"\"\"Create table data for parallel coordinate visualization\"\"\"\n",
        "\n",
        "    if data['trials'] is None:\n",
        "        print(\"No trials data available\")\n",
        "        return None\n",
        "\n",
        "    df = data['trials'].copy()\n",
        "\n",
        "    # Extract parameter columns\n",
        "    param_cols = [col for col in df.columns if col.startswith('params_')]\n",
        "\n",
        "    if not param_cols:\n",
        "        print(\"No parameter columns found in trials data\")\n",
        "        return None\n",
        "\n",
        "    # Create base table\n",
        "    parallel_table = df[['number', 'value'] + param_cols].copy()\n",
        "\n",
        "    # Convert objective to positive BLEU\n",
        "    parallel_table['BLEU_Score'] = -parallel_table['value']\n",
        "\n",
        "    # Add performance categories\n",
        "    bleu_scores = parallel_table['BLEU_Score'].dropna()\n",
        "    parallel_table['Performance_Quartile'] = pd.qcut(\n",
        "        parallel_table['BLEU_Score'],\n",
        "        q=4,\n",
        "        labels=['Q1_Worst', 'Q2_Below_Avg', 'Q3_Above_Avg', 'Q4_Best']\n",
        "    )\n",
        "\n",
        "    # Add top performers flag\n",
        "    top_10_threshold = bleu_scores.quantile(0.9)\n",
        "    parallel_table['Is_Top_10_Percent'] = parallel_table['BLEU_Score'] >= top_10_threshold\n",
        "\n",
        "    # Normalize parameters to 0-1 scale for better visualization\n",
        "    param_cols_clean = [col.replace('params_', '') for col in param_cols]\n",
        "    for i, col in enumerate(param_cols):\n",
        "        col_clean = param_cols_clean[i]\n",
        "        parallel_table[f'{col_clean}_normalized'] = (\n",
        "            (parallel_table[col] - parallel_table[col].min()) /\n",
        "            (parallel_table[col].max() - parallel_table[col].min())\n",
        "        )\n",
        "\n",
        "    return parallel_table\n",
        "\n",
        "# 4. CONTOUR PLOT DATA TABLE\n",
        "\n",
        "def create_contour_data_table(data, param1, param2):\n",
        "    \"\"\"Create table data for specific parameter pair contour plot\"\"\"\n",
        "\n",
        "    if data['trials'] is None:\n",
        "        print(\"No trials data available\")\n",
        "        return None\n",
        "\n",
        "    df = data['trials'].copy()\n",
        "\n",
        "    param1_col = f'params_{param1}'\n",
        "    param2_col = f'params_{param2}'\n",
        "\n",
        "    if param1_col not in df.columns or param2_col not in df.columns:\n",
        "        print(f\"Parameters {param1} or {param2} not found in trials data\")\n",
        "        return None\n",
        "\n",
        "    # Create contour table\n",
        "    contour_table = pd.DataFrame({\n",
        "        'Trial_Number': df['number'],\n",
        "        f'{param1}': df[param1_col],\n",
        "        f'{param2}': df[param2_col],\n",
        "        'BLEU_Score': -df['value'],\n",
        "        'Objective_Value': df['value']\n",
        "    })\n",
        "\n",
        "    # Remove NaN values\n",
        "    contour_table = contour_table.dropna()\n",
        "\n",
        "    # Add performance binning\n",
        "    contour_table['Performance_Bin'] = pd.cut(\n",
        "        contour_table['BLEU_Score'],\n",
        "        bins=5,\n",
        "        labels=['Very_Poor', 'Poor', 'Average', 'Good', 'Excellent']\n",
        "    )\n",
        "\n",
        "    # Add distance from best point\n",
        "    best_idx = contour_table['BLEU_Score'].idxmax()\n",
        "    best_param1 = contour_table.loc[best_idx, param1]\n",
        "    best_param2 = contour_table.loc[best_idx, param2]\n",
        "\n",
        "    contour_table['Distance_From_Best'] = np.sqrt(\n",
        "        (contour_table[param1] - best_param1)**2 +\n",
        "        (contour_table[param2] - best_param2)**2\n",
        "    )\n",
        "\n",
        "    # Add grid coordinates for contour plotting\n",
        "    param1_grid = np.linspace(contour_table[param1].min(), contour_table[param1].max(), 50)\n",
        "    param2_grid = np.linspace(contour_table[param2].min(), contour_table[param2].max(), 50)\n",
        "\n",
        "    contour_table['Grid_X_Idx'] = pd.cut(contour_table[param1], bins=param1_grid, labels=False)\n",
        "    contour_table['Grid_Y_Idx'] = pd.cut(contour_table[param2], bins=param2_grid, labels=False)\n",
        "\n",
        "    return contour_table\n",
        "\n",
        "# 5. SLICE PLOT DATA TABLE\n",
        "\n",
        "def create_slice_data_table(data, param_name):\n",
        "    \"\"\"Create table data for parameter slice plot\"\"\"\n",
        "\n",
        "    if data['tpe_distributions'] is None or data['trials'] is None:\n",
        "        print(\"Required data not available\")\n",
        "        return None\n",
        "\n",
        "    df = data['trials'].copy()\n",
        "    param_col = f'params_{param_name}'\n",
        "\n",
        "    if param_col not in df.columns:\n",
        "        print(f\"Parameter {param_name} not found in trials data\")\n",
        "        return None\n",
        "\n",
        "    # Create slice table\n",
        "    slice_table = pd.DataFrame({\n",
        "        'Trial_Number': df['number'],\n",
        "        f'{param_name}': df[param_col],\n",
        "        'BLEU_Score': -df['value'],\n",
        "        'Objective_Value': df['value']\n",
        "    })\n",
        "\n",
        "    # Remove NaN values\n",
        "    slice_table = slice_table.dropna()\n",
        "\n",
        "    # Sort by parameter value\n",
        "    slice_table = slice_table.sort_values(param_name)\n",
        "\n",
        "    # Add moving average for trend\n",
        "    slice_table['BLEU_Moving_Avg'] = slice_table['BLEU_Score'].rolling(window=5, center=True).mean()\n",
        "\n",
        "    # Add TPE distribution info if available\n",
        "    tpe_dist = data['tpe_distributions']\n",
        "    param_dist = tpe_dist[tpe_dist['parameter'] == param_name]\n",
        "\n",
        "    if not param_dist.empty:\n",
        "        good_mean = param_dist['good_mean'].iloc[0]\n",
        "        poor_mean = param_dist['poor_mean'].iloc[0]\n",
        "\n",
        "        slice_table['TPE_Good_Mean'] = good_mean\n",
        "        slice_table['TPE_Poor_Mean'] = poor_mean\n",
        "        slice_table['Distance_From_Good_Mean'] = abs(slice_table[param_name] - good_mean)\n",
        "        slice_table['Distance_From_Poor_Mean'] = abs(slice_table[param_name] - poor_mean)\n",
        "        slice_table['Closer_To_Good_Region'] = (\n",
        "            slice_table['Distance_From_Good_Mean'] < slice_table['Distance_From_Poor_Mean']\n",
        "        )\n",
        "\n",
        "    # Add performance percentiles\n",
        "    slice_table['Performance_Percentile'] = slice_table['BLEU_Score'].rank(pct=True) * 100\n",
        "\n",
        "    # Bin parameter values\n",
        "    slice_table['Parameter_Bin'] = pd.cut(\n",
        "        slice_table[param_name],\n",
        "        bins=10,\n",
        "        labels=[f'Bin_{i+1}' for i in range(10)]\n",
        "    )\n",
        "\n",
        "    return slice_table\n",
        "\n",
        "# 6. MASTER FUNCTION - CREATE ALL TABLES\n",
        "\n",
        "def create_all_visualization_tables(base_path):\n",
        "    \"\"\"Create all visualization tables and save them\"\"\"\n",
        "\n",
        "    print(\"Loading analysis data...\")\n",
        "    data = load_all_analysis_data(base_path)\n",
        "\n",
        "     #  Perbaikan: cek data yang tidak None dan tidak kosong\n",
        "    if not any(df is not None and not df.empty for df in data.values()):\n",
        "        print(\" No analysis data found or all files are empty!\")\n",
        "        return None\n",
        "\n",
        "    #  Debug info - tunjukkan status tiap file\n",
        "    print(\"\\n--- FILE STATUS ---\")\n",
        "    for name, df in data.items():\n",
        "        if df is None:\n",
        "            print(f\" {name}: not loaded\")\n",
        "        elif df.empty:\n",
        "            print(f\" {name}: loaded but empty\")\n",
        "        else:\n",
        "            print(f\" {name}: loaded with shape {df.shape}\")\n",
        "    print(\"-------------------\\n\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Optimization History Table\n",
        "    print(\"\\n1. Creating Optimization History Table...\")\n",
        "    results['optimization_history'] = create_optimization_history_table(data)\n",
        "\n",
        "    # 2. Parameter Importance Table\n",
        "    print(\"2. Creating Parameter Importance Table...\")\n",
        "    results['parameter_importance'] = create_parameter_importance_table(data)\n",
        "\n",
        "    # 3. Parallel Coordinate Table\n",
        "    print(\"3. Creating Parallel Coordinate Table...\")\n",
        "    results['parallel_coordinate'] = create_parallel_coordinate_table(data)\n",
        "\n",
        "    # 4. Contour Tables - for all parameter pairs\n",
        "    if data['trials'] is not None:\n",
        "        print(\"4. Creating Contour Data Tables...\")\n",
        "        param_cols = [col.replace('params_', '') for col in data['trials'].columns if col.startswith('params_')]\n",
        "        results['contour_tables'] = {}\n",
        "\n",
        "        from itertools import combinations\n",
        "        for param1, param2 in combinations(param_cols, 2):\n",
        "            print(f\"   Creating contour table for {param1} vs {param2}\")\n",
        "            results['contour_tables'][f'{param1}_vs_{param2}'] = create_contour_data_table(data, param1, param2)\n",
        "\n",
        "    # 5. Slice Tables - for each parameter\n",
        "    if data['trials'] is not None:\n",
        "        print(\"5. Creating Slice Data Tables...\")\n",
        "        param_cols = [col.replace('params_', '') for col in data['trials'].columns if col.startswith('params_')]\n",
        "        results['slice_tables'] = {}\n",
        "\n",
        "        for param in param_cols:\n",
        "            print(f\"   Creating slice table for {param}\")\n",
        "            results['slice_tables'][param] = create_slice_data_table(data, param)\n",
        "\n",
        "    # Save all tables to CSV\n",
        "    print(\"\\nSaving all tables to CSV...\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    for table_name, table_data in results.items():\n",
        "        if table_data is not None and not isinstance(table_data, dict):\n",
        "            filename = os.path.join(base_path, f'viz_table_{table_name}_{timestamp}.csv')\n",
        "            table_data.to_csv(filename, index=False)\n",
        "            print(f\"✓ Saved: {filename}\")\n",
        "\n",
        "    # Save contour and slice tables\n",
        "    if 'contour_tables' in results:\n",
        "        for contour_name, contour_data in results['contour_tables'].items():\n",
        "            if contour_data is not None:\n",
        "                filename = os.path.join(base_path, f'viz_table_contour_{contour_name}_{timestamp}.csv')\n",
        "                contour_data.to_csv(filename, index=False)\n",
        "                print(f\"✓ Saved: {filename}\")\n",
        "\n",
        "    if 'slice_tables' in results:\n",
        "        for slice_name, slice_data in results['slice_tables'].items():\n",
        "            if slice_data is not None:\n",
        "                filename = os.path.join(base_path, f'viz_table_slice_{slice_name}_{timestamp}.csv')\n",
        "                slice_data.to_csv(filename, index=False)\n",
        "                print(f\"✓ Saved: {filename}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 7. SUMMARY STATISTICS FOR EACH VISUALIZATION\n",
        "\n",
        "def create_visualization_summary_stats(results):\n",
        "    \"\"\"Create summary statistics for each visualization table\"\"\"\n",
        "\n",
        "    summary_stats = {}\n",
        "\n",
        "    # Optimization History Stats\n",
        "    if results.get('optimization_history') is not None:\n",
        "        hist_data = results['optimization_history']\n",
        "        summary_stats['optimization_history'] = {\n",
        "            'total_trials': len(hist_data),\n",
        "            'best_bleu_score': hist_data['BLEU_Score'].max(),\n",
        "            'worst_bleu_score': hist_data['BLEU_Score'].min(),\n",
        "            'average_bleu_score': hist_data['BLEU_Score'].mean(),\n",
        "            'improvement_rate': (hist_data['Is_Improvement'].sum() / len(hist_data)) * 100,\n",
        "            'convergence_trial': hist_data[hist_data['BLEU_Score'] == hist_data['BLEU_Score'].max()]['Trial_Number'].iloc[0]\n",
        "        }\n",
        "\n",
        "    # Parameter Importance Stats\n",
        "    if results.get('parameter_importance') is not None:\n",
        "        param_data = results['parameter_importance']\n",
        "        summary_stats['parameter_importance'] = {\n",
        "            'most_important_param': param_data.iloc[0]['parameter'],\n",
        "            'least_important_param': param_data.iloc[-1]['parameter'],\n",
        "            'top_3_params_contribute': param_data.head(3)['Importance_Percentage'].sum(),\n",
        "            'parameters_above_10_percent': len(param_data[param_data['Importance_Percentage'] > 10])\n",
        "        }\n",
        "\n",
        "    # Parallel Coordinate Stats\n",
        "    if results.get('parallel_coordinate') is not None:\n",
        "        par_data = results['parallel_coordinate']\n",
        "        summary_stats['parallel_coordinate'] = {\n",
        "            'top_10_percent_trials': par_data['Is_Top_10_Percent'].sum(),\n",
        "            'best_quartile_trials': len(par_data[par_data['Performance_Quartile'] == 'Q4_Best']),\n",
        "            'parameter_count': len([col for col in par_data.columns if col.endswith('_normalized')])\n",
        "        }\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "\"\"\"Example usage of all functions\"\"\"\n",
        "\n",
        "# Set your base path where CSV files are located\n",
        "BASE_PATH = LOG_PATH\n",
        "\n",
        "# Create all visualization tables\n",
        "print(\"Creating all visualization tables...\")\n",
        "results = create_all_visualization_tables(BASE_PATH)\n",
        "\n",
        "if results:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"VISUALIZATION TABLES CREATED SUCCESSFULLY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create summary statistics\n",
        "    summary_stats = create_visualization_summary_stats(results)\n",
        "\n",
        "    # Print summary\n",
        "    for viz_name, stats in summary_stats.items():\n",
        "        print(f\"\\n{viz_name.upper()} SUMMARY:\")\n",
        "        for stat_name, stat_value in stats.items():\n",
        "            print(f\"  {stat_name}: {stat_value}\")\n",
        "\n",
        "    # Display sample data from each table\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE DATA FROM EACH TABLE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for table_name, table_data in results.items():\n",
        "        if table_data is not None and not isinstance(table_data, dict):\n",
        "            print(f\"\\n{table_name.upper()} (First 5 rows):\")\n",
        "            print(table_data.head())\n",
        "            print(f\"Shape: {table_data.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to create visualization tables!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMjN4t13tAf"
      },
      "source": [
        "### Plot Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_0MFT_I3vye"
      },
      "outputs": [],
      "source": [
        "import optuna.visualization.matplotlib as vis\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# 1. Optimization History Plot\n",
        "ax1 = vis.plot_optimization_history(study)\n",
        "fig1 = ax1.figure\n",
        "fig1.set_size_inches(14, 8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OPTUNA_PATH, \"optuna_history_bleu4_full.png\"))\n",
        "plt.show()\n",
        "\n",
        "# 2. Param Importances Plot\n",
        "ax2 = vis.plot_param_importances(study)\n",
        "fig2 = ax2.figure\n",
        "fig2.set_size_inches(14, 8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OPTUNA_PATH, \"optuna_param_importance_full.png\"))\n",
        "plt.show()\n",
        "\n",
        "# 3. Parallel Coordinate Plot\n",
        "ax4 = vis.plot_parallel_coordinate(study)\n",
        "fig4 = ax4.figure\n",
        "fig4.set_size_inches(14, 8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OPTUNA_PATH, \"optuna_parallel_coordinate.png\"))\n",
        "plt.show()\n",
        "\n",
        "# 4. Contour Plot for all param pairs\n",
        "all_params = list(study.best_trial.params.keys())\n",
        "param_pairs = list(itertools.combinations(all_params, 2))\n",
        "\n",
        "for param1, param2 in param_pairs:\n",
        "    ax = vis.plot_contour(study, params=[param1, param2])\n",
        "    if isinstance(ax, plt.Axes):\n",
        "        fig = ax.figure\n",
        "    else:\n",
        "        fig = ax[0, 0].figure\n",
        "    fig.set_size_inches(8, 6)\n",
        "    plt.suptitle(f'Contour Plot: {param1} vs {param2}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fname = f\"optuna_contour_{param1}_vs_{param2}.png\"\n",
        "    plt.savefig(os.path.join(OPTUNA_PATH, fname))\n",
        "    plt.show()\n",
        "\n",
        "# 5. Slice Plot for each param\n",
        "for param in all_params:\n",
        "    ax = vis.plot_slice(study, params=[param])\n",
        "    if isinstance(ax, plt.Axes):\n",
        "        fig = ax.figure\n",
        "    else:\n",
        "        fig = ax[0].figure\n",
        "    fig.set_size_inches(8, 6)\n",
        "    plt.title(f\"Slice Plot for '{param}'\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OPTUNA_PATH, f\"optuna_slice_{param}.png\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Finetune"
      ],
      "metadata": {
        "id": "EPJ8itJIVESF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEmq4lfZGZy3"
      },
      "source": [
        "## Finetuning Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq6GdJFC_nMH"
      },
      "outputs": [],
      "source": [
        "# # Fine-tuning Process\n",
        "# Melatih model T5 dengan dataset yang telah disiapkan\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = T5Config.from_pretrained(\"t5-base\")\n",
        "# config.dropout_rate = 0.1254686517839752\n",
        "# MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", config=config).to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Inisialisasi optimizer\n",
        "current_lr=4e-05\n",
        "current_wd=1e-05\n",
        "OPTIMIZER = AdamW(MODEL.parameters(), lr=current_lr, eps=1e-8, weight_decay=current_wd)\n",
        "best_val_loss = float('inf')\n",
        "batch_size = 12\n",
        "\n",
        "# Inisialisasi data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ggeMT1zdiGz"
      },
      "source": [
        "## T5-BASE FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9erIfIddlcb"
      },
      "outputs": [],
      "source": [
        "# List untuk menyimpan loss\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "loss_file = os.path.join(LOG_PATH, 'losses.txt')\n",
        "with open(loss_file, 'w') as f:\n",
        "    f.write(f'\\nHyperparmater Combination:\\nLearning rate: {current_lr} \\nBatch size: {batch_size} \\nWeight Decay: {current_wd}\\nDropout: {config.dropout_rate}\\n')\n",
        "    print(f'\\nHyperparmater Combination:\\nLearning rate: {current_lr} \\nBatch size: {batch_size} \\nWeight Decay: {current_wd} \\nDropout: {config.dropout_rate}\\n')\n",
        "\n",
        "    for epoch in range(10):\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_batch_count = 0\n",
        "        val_batch_count = 0\n",
        "        MODEL.train()\n",
        "\n",
        "        for batch in tqdm(train_loader, desc='Training batches'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            outputs = MODEL(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=decoder_attention_mask\n",
        "            )\n",
        "\n",
        "            OPTIMIZER.zero_grad()\n",
        "            outputs.loss.backward()\n",
        "            OPTIMIZER.step()\n",
        "            train_loss += outputs.loss.item()\n",
        "            train_batch_count += 1\n",
        "\n",
        "        MODEL.eval()\n",
        "        for batch in tqdm(val_loader, desc='Validation batches'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = MODEL(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,\n",
        "                    decoder_attention_mask=decoder_attention_mask\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "            val_batch_count += 1\n",
        "\n",
        "        avg_train_loss = train_loss / train_batch_count\n",
        "        avg_val_loss = val_loss / val_batch_count\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "        else:\n",
        "            print(f'\\nEarly stopping')\n",
        "            break\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        f.write(f'\\nEpoch {epoch+1}/10 -> Train loss: {avg_train_loss}\\tValidation loss: {avg_val_loss}\\n')\n",
        "        print(f'\\n{epoch+1}/10 -> Train loss: {avg_train_loss}\\tValidation loss: {avg_val_loss}\\n')\n",
        "\n",
        "\n",
        "# Simpan model dan tokenizer ke Google Drive\n",
        "MODEL.save_pretrained(MODEL_PATH)\n",
        "TOKENIZER.save_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "# Simpan plot grafik loss ke Google Drive\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plot_path = os.path.join(PROJECT_PATH, 'loss_plot.png')\n",
        "plt.savefig(plot_path)  # Simpan plot sebagai file PNG\n",
        "plt.show()\n",
        "\n",
        "print(f'Final Train loss: {avg_train_loss}\\tFinal Validation loss: {avg_val_loss}')\n",
        "print(f'Model saved to: {MODEL_PATH}')\n",
        "print(f'Tokenizer saved to: {TOKENIZER_PATH}')\n",
        "print(f'Loss log saved to: {loss_file}')\n",
        "print(f'Loss plot saved to: {plot_path}')\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj1zp-cy_2x4"
      },
      "outputs": [],
      "source": [
        "# Testing the Fine-tuned T5-base Model\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
        "\n",
        "print(f\"Loaded fine-tuned T5-base model\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEADf4b4_7XP"
      },
      "outputs": [],
      "source": [
        "## Example 1\n",
        "# Menguji model dengan contoh sederhana\n",
        "\n",
        "context = \"Sushi is a special food from Japan. It uses rice with vinegar and fresh fish. Sometimes, people add cucumber, spicy sauce, or even aloe vera. Why do people eat raw fish? Because it is healthy, fresh, and light to eat. So, sushi is not only yummy, but also good for your body!\"\n",
        "answer = \"Japan\"\n",
        "text = f\"context: {context} answer: {answer} </s>\"\n",
        "print(\"Input:\", text)\n",
        "\n",
        "# Tokenisasi input\n",
        "encoding = tokenizer.encode_plus(text, max_length=input_max_len, padding=True, return_tensors=\"pt\")\n",
        "print(\"Encoding keys:\", encoding.keys())\n",
        "input_ids, attention_mask = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "# Menghasilkan pertanyaan\n",
        "model.eval()\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=target_max_len,\n",
        "    early_stopping=True,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "# Mendekode dan mencetak hasil\n",
        "print(\"\\nGenerated Questions:\")\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6akXBrxW_7-X"
      },
      "outputs": [],
      "source": [
        "## Example 2\n",
        "# Menguji model dengan konteks yang lebih kompleks\n",
        "\n",
        "context = \"\"\"\n",
        "I live in a simple, comfortable house on a mountain slope, facing a road. A big mosque is on the left, used for praying and Quran lessons. Kids play in a wide yard in front. My two-story house is painted ivory white with brown window frames and door. It suits four to six people. There’s a terrace for relaxing, a living room upon entry, and a family room on the left where we talk or watch TV. A door in the family room leads to the kitchen, dining room, and bathroom. The master bedroom, with a bathroom, is on the right of the dining room. The second floor has two bedrooms and a balcony with a beautiful mountain view.\n",
        "\"\"\"\n",
        "answer = \"balcony\"\n",
        "text1 = f\"context: {context} answer: {answer} </s>\"\n",
        "print(\"\\nInput:\", text1)\n",
        "\n",
        "# Tokenisasi input\n",
        "encoding = tokenizer.encode_plus(text1, max_length=input_max_len, padding=True, return_tensors=\"pt\")\n",
        "print(\"Encoding keys:\", encoding.keys())\n",
        "input_ids, attention_mask = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "# Menghasilkan pertanyaan\n",
        "model.eval()\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=target_max_len,\n",
        "    early_stopping=True,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "# Mendekode dan mencetak hasil\n",
        "print(\"\\nGenerated Questions:\")\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgdXOQvdAbMf"
      },
      "source": [
        "## Processing the Evaluation Metrics - Same logic as original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3TAhQuq_9eA"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Processing the Evaluation Metrics - Same logic as original\n",
        "# ================================\n",
        "\n",
        "# Loading the Context from Tokenized Test Dataset\n",
        "decoded_inputs = []\n",
        "def decode_and_write_to_txt(dataset, tokenizer, output_file):\n",
        "    for i in range(len(dataset)):\n",
        "        decoded_input = tokenizer.decode(dataset.inputs[i]['input_ids'].squeeze(), skip_special_tokens=True)\n",
        "        decoded_inputs.append(decoded_input)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for decoded_input in decoded_inputs:\n",
        "            f.write(decoded_input + '\\n')\n",
        "\n",
        "src_test_preds_path = os.path.join(PROJECT_PATH, 'src_test_preds.txt')  # MODIFIED: Different filename\n",
        "decode_and_write_to_txt(test_dataset, tokenizer, src_test_preds_path)\n",
        "\n",
        "# Process input to save only context\n",
        "def preprocess_text(text):\n",
        "    context_part = text.split('context:')[1].strip()\n",
        "    cleaned_context = context_part.split('answers:')[0].strip()\n",
        "    cleaned_context = f'\"{cleaned_context}\"'\n",
        "    return cleaned_context\n",
        "\n",
        "src_test_preds_processed_path = os.path.join(PROJECT_PATH, 'src_test_preds_processed.txt')\n",
        "with open(src_test_preds_path, 'r', encoding='utf-8') as infile, open(src_test_preds_processed_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        cleaned_context = preprocess_text(line)\n",
        "        outfile.write(cleaned_context + '\\n')\n",
        "\n",
        "with open(src_test_preds_processed_path, 'r') as f:\n",
        "    print(f'Processed context length: {len(f.readlines())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoUrSlnwAFF4"
      },
      "outputs": [],
      "source": [
        "# Loading the Reference Target (Question) from Tokenized Test Dataset\n",
        "decoded_target = []\n",
        "def decode_and_write_to_txt(dataset, tokenizer, output_file):\n",
        "    for i in range(len(dataset)):\n",
        "        decoded_input = tokenizer.decode(dataset.targets[i]['input_ids'].squeeze(), skip_special_tokens=True)\n",
        "        decoded_target.append(decoded_input)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for decoded_input in decoded_target:\n",
        "            f.write(decoded_input + '\\n')\n",
        "\n",
        "test_ref_path = os.path.join(PROJECT_PATH, 'test_ref.txt')\n",
        "decode_and_write_to_txt(test_dataset, tokenizer, test_ref_path)\n",
        "\n",
        "# Process targets for text cleaning\n",
        "test_ref_processed_path = os.path.join(PROJECT_PATH, 'test_ref_processed.txt')\n",
        "dataset = []\n",
        "with open(test_ref_path, encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        dataset.append(line.strip())\n",
        "\n",
        "for idx, data in enumerate(dataset):\n",
        "    if data.endswith('?'):\n",
        "        dataset[idx] = data[:-1] + ' ?'\n",
        "    else:\n",
        "        dataset[idx] = data + ' ?'\n",
        "    if data.find(\"'s\") != -1:\n",
        "        dataset[idx] = dataset[idx].replace(\"'s\", \" 's\")\n",
        "    dataset[idx] = dataset[idx].replace('  ', ' ')\n",
        "    if data.find('question:') == 0:\n",
        "        dataset[idx] = data[10:]\n",
        "    if data[0] == ',':\n",
        "        dataset[idx] = data[1:]\n",
        "    dataset[idx] = dataset[idx].lstrip()\n",
        "\n",
        "with open(test_ref_processed_path, 'w', encoding='utf-8') as f:\n",
        "    for data in dataset:\n",
        "        f.write(data.strip() + '\\n')\n",
        "\n",
        "with open(test_ref_processed_path, 'r') as f:\n",
        "    print(f'Processed reference length: {len(f.readlines())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM1Eu4YFAYzn"
      },
      "source": [
        "## Generate predictions with T5-small optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SifH75ExAOCo"
      },
      "outputs": [],
      "source": [
        "\n",
        "generated_questions = []\n",
        "\n",
        "for i in tqdm(range(len(test_dataset)), desc=\"Generating questions\"):\n",
        "    input_ids = test_dataset.inputs[i]['input_ids'].to(device)\n",
        "    attention_mask = test_dataset.inputs[i]['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=96,\n",
        "            num_beams=4,              # MODIFIED: Added beam search for better quality\n",
        "            no_repeat_ngram_size=2,   # MODIFIED: Prevent repetition\n",
        "            repetition_penalty=1.2,   # MODIFIED: Reduce repetitive outputs\n",
        "            length_penalty=1.0,       # MODIFIED: Encourage appropriate length\n",
        "            early_stopping=True       # MODIFIED: Stop when EOS is generated\n",
        "        )\n",
        "    generated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_questions.append(generated_question)\n",
        "\n",
        "test_preds_path = os.path.join(PROJECT_PATH, 'test_preds.txt')\n",
        "with open(test_preds_path, 'w', encoding='utf-8') as f:\n",
        "    for question in generated_questions:\n",
        "        f.write(question + '\\n')\n",
        "\n",
        "# Process predictions\n",
        "test_preds_processed_path = os.path.join(PROJECT_PATH, 'test_preds_processed.txt')\n",
        "dataset = []\n",
        "with open(test_preds_path, encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        dataset.append(line.strip())\n",
        "\n",
        "for idx, data in enumerate(dataset):\n",
        "    if data.endswith('?'):\n",
        "        dataset[idx] = data[:-1] + ' ?'\n",
        "    else:\n",
        "        dataset[idx] = data + ' ?'\n",
        "    if data.find(\"'s\") != -1:\n",
        "        dataset[idx] = dataset[idx].replace(\"'s\", \" 's\")\n",
        "    dataset[idx] = dataset[idx].replace('  ', ' ')\n",
        "    if data.find('question:') == 0:\n",
        "        dataset[idx] = data[10:]\n",
        "    if data[0] == ',':\n",
        "        dataset[idx] = data[1:]\n",
        "    dataset[idx] = dataset[idx].lstrip()\n",
        "\n",
        "with open(test_preds_processed_path, 'w', encoding='utf-8') as f:\n",
        "    for data in dataset:\n",
        "        f.write(data.strip() + '\\n')\n",
        "\n",
        "with open(test_preds_processed_path, 'r') as f:\n",
        "    print(f'Processed predictions length: {len(f.readlines())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es7klFEUAXeA"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAfPxVuL-oV5"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "context_full_path = os.path.join(PROJECT_PATH, 'src_test_preds.txt')\n",
        "test_ref_processed_path = os.path.join(PROJECT_PATH, 'test_ref_processed.txt')\n",
        "test_preds_processed_path = os.path.join(PROJECT_PATH, 'test_preds_processed.txt')\n",
        "evaluation_result_path = os.path.join(EVALUATION_PATH, 'evaluation_result.txt')\n",
        "evaluation_plot_path = os.path.join(EVALUATION_PATH, 'evaluation_plot.png')\n",
        "metric_correlation_plot_path = os.path.join(EVALUATION_PATH, 'metric_correlation_plot.png')\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    return [line.strip() for line in lines]\n",
        "\n",
        "def calculate_bleu(candidate, references):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_1 = sentence_bleu(references, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
        "    bleu_2 = sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
        "    bleu_3 = sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
        "    bleu_4 = sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
        "    return bleu_1, bleu_2, bleu_3, bleu_4\n",
        "\n",
        "def calculate_meteor(candidate, references):\n",
        "    candidate_tok = candidate.split()\n",
        "    references_tok = [ref.split() for ref in references]\n",
        "    return meteor_score(references_tok, candidate_tok)\n",
        "\n",
        "def calculate_rouge_l(candidate, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(references[0], candidate)\n",
        "    return scores['rougeL'].fmeasure\n",
        "\n",
        "# Load all necessary data\n",
        "contexts_answers = read_file(context_full_path)\n",
        "references = read_file(test_ref_processed_path)\n",
        "candidates = read_file(test_preds_processed_path)\n",
        "\n",
        "# Extract context and answer\n",
        "contexts = []\n",
        "answers = []\n",
        "for line in contexts_answers:\n",
        "    if line.startswith(\"context:\"):\n",
        "        ctx = line.split(\"context:\")[1].split(\"answers:\")[0].strip().strip('\"')\n",
        "        ans = line.split(\"answers:\")[1].strip()\n",
        "        contexts.append(ctx)\n",
        "        answers.append(ans)\n",
        "    else:\n",
        "        contexts.append(\"\")  # fallback\n",
        "        answers.append(\"\")\n",
        "\n",
        "# Compute all metrics\n",
        "bleu_scores = []\n",
        "meteor_scores = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "for ref, cand in tqdm(zip(references, candidates), desc=\"Computing metrics\"):\n",
        "    bleu_1, bleu_2, bleu_3, bleu_4 = calculate_bleu(cand, [ref])\n",
        "    meteor = calculate_meteor(cand, [ref])\n",
        "    rouge_l = calculate_rouge_l(cand, [ref])\n",
        "\n",
        "    bleu_scores.append((bleu_1, bleu_2, bleu_3, bleu_4))\n",
        "    meteor_scores.append(meteor)\n",
        "    rouge_l_scores.append(rouge_l)\n",
        "\n",
        "# Calculate averages\n",
        "avg_bleu_1 = sum([score[0] for score in bleu_scores]) / len(bleu_scores) * 100\n",
        "avg_bleu_2 = sum([score[1] for score in bleu_scores]) / len(bleu_scores) * 100\n",
        "avg_bleu_3 = sum([score[2] for score in bleu_scores]) / len(bleu_scores) * 100\n",
        "avg_bleu_4 = sum([score[3] for score in bleu_scores]) / len(bleu_scores) * 100\n",
        "avg_meteor = sum(meteor_scores) / len(meteor_scores) * 100\n",
        "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) * 100\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=\"*60)\n",
        "print(f'Average BLEU-1: {avg_bleu_1:.4f}%')\n",
        "print(f'Average BLEU-2: {avg_bleu_2:.4f}%')\n",
        "print(f'Average BLEU-3: {avg_bleu_3:.4f}%')\n",
        "print(f'Average BLEU-4: {avg_bleu_4:.4f}%')\n",
        "print(f'Average METEOR: {avg_meteor:.4f}%')\n",
        "print(f'Average ROUGE-L: {avg_rouge_l:.4f}%')\n",
        "\n",
        "# Save results to file\n",
        "with open(evaluation_result_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\"*50 + \"\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\")\n",
        "    f.write(f'Average BLEU-1: {avg_bleu_1:.4f}%\\n')\n",
        "    f.write(f'Average BLEU-2: {avg_bleu_2:.4f}%\\n')\n",
        "    f.write(f'Average BLEU-3: {avg_bleu_3:.4f}%\\n')\n",
        "    f.write(f'Average BLEU-4: {avg_bleu_4:.4f}%\\n')\n",
        "    f.write(f'Average METEOR: {avg_meteor:.4f}%\\n')\n",
        "    f.write(f'Average ROUGE-L: {avg_rouge_l:.4f}%\\n')\n",
        "\n",
        "print(f'T5-Small evaluation results saved to: {evaluation_result_path}')\n",
        "\n",
        "# Visualization Bar Chart\n",
        "labels = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR', 'ROUGE-L']\n",
        "scores = [avg_bleu_1, avg_bleu_2, avg_bleu_3, avg_bleu_4, avg_meteor, avg_rouge_l]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "bars = plt.bar(labels, scores, color=['#4e79a7', '#59a14f', '#9c755f', '#f28e2b', '#e15759', '#76b7b2'])\n",
        "plt.ylim(0, 100)\n",
        "plt.title('Question Generation - Evaluation Metrics (%)')\n",
        "plt.ylabel('Score (%)')\n",
        "\n",
        "# Add values on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(evaluation_plot_path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f'T5-Small evaluation plot saved to: {evaluation_plot_path}')\n",
        "\n",
        "# Show 5 example outputs\n",
        "for i in range(5):\n",
        "    print(f\"Example #{i+1}\")\n",
        "    print(f\"Context   : {contexts[i][:100]}...\" if len(contexts[i]) > 100 else f\"Context   : {contexts[i]}\")\n",
        "    print(f\"Answer    : {answers[i]}\")\n",
        "    print(f\"Reference : {references[i]}\")\n",
        "    print(f\"T5-Small  : {candidates[i]}\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Metric correlation plot\n",
        "df = pd.DataFrame(bleu_scores, columns=['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4'])\n",
        "df['METEOR'] = meteor_scores\n",
        "df['ROUGE-L'] = rouge_l_scores\n",
        "\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', center=0)\n",
        "plt.title('Correlation between Evaluation Metrics')\n",
        "plt.tight_layout()\n",
        "plt.savefig(metric_correlation_plot_path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f'T5-Small metric correlation plot saved to: {metric_correlation_plot_path}')\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for split, data in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
        "    print(f\"\\n{split} Dataset:\")\n",
        "    print(f\"  Size: {len(data):,} samples\")\n",
        "\n",
        "    # Target lengths\n",
        "    target_lengths = [len(TOKENIZER.encode(f\"question: {q}\")) for q in data['question']]\n",
        "    print(f\"  Target length - Mean: {np.mean(target_lengths):.2f}, Max: {np.max(target_lengths)}\")\n",
        "\n",
        "    # Context word counts\n",
        "    context_words = [len(str(context).split()) for context in data['context']]\n",
        "    print(f\"  Context words - Mean: {np.mean(context_words):.2f}, Max: {np.max(context_words)}\")\n",
        "\n",
        "    # Answer word counts\n",
        "    answer_words = [len(str(answer).split()) for answer in data['answers']]\n",
        "    print(f\"  Answer words - Mean: {np.mean(answer_words):.2f}, Max: {np.max(answer_words)}\")\n",
        "\n",
        "    # Question word counts\n",
        "    question_words = [len(str(q).split()) for q in data['question']]\n",
        "    print(f\"  Question words - Mean: {np.mean(question_words):.2f}, Max: {np.max(question_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8niDKoLUyFr"
      },
      "source": [
        "# Loop (Finetune, Decode, Evaluate) Multi hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "K3lE1AKnUyFs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Top 10 HPO1 loop 1 TPEsampler 64 kombinasi\n",
        "top10_hparams1 = [\n",
        "    {\"learning_rate\": 3e-05, \"batch_size\": 8, \"weight_decay\": 0.0001},\n",
        "    {\"learning_rate\": 3e-05, \"batch_size\": 6, \"weight_decay\": 1e-05},\n",
        "    {\"learning_rate\": 4e-05, \"batch_size\": 6, \"weight_decay\": 0.0001},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 0.0001},\n",
        "    {\"learning_rate\": 3e-05, \"batch_size\": 6, \"weight_decay\": 0.0001},\n",
        "    {\"learning_rate\": 3e-05, \"batch_size\": 12, \"weight_decay\": 5e-05},\n",
        "    {\"learning_rate\": 4e-05, \"batch_size\": 12, \"weight_decay\": 1e-05},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 0.0001},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 5e-05},\n",
        "    {\"learning_rate\": 4e-05, \"batch_size\": 12, \"weight_decay\": 1e-05},\n",
        "]\n",
        "\n",
        "# top 10 HPO1 loop 2 Gridsearch\n",
        "top10_hparams2 = [\n",
        "    {\"learning_rate\": 3.2e-05, \"batch_size\": 8,  \"weight_decay\": 1.1e-4},\n",
        "    {\"learning_rate\": 3e-05,   \"batch_size\": 7,  \"weight_decay\": 9e-05},\n",
        "    {\"learning_rate\": 3e-05,   \"batch_size\": 6,  \"weight_decay\": 1e-4},\n",
        "    {\"learning_rate\": 3.5e-05, \"batch_size\": 6,  \"weight_decay\": 9e-05},\n",
        "    {\"learning_rate\": 3.2e-05, \"batch_size\": 6,  \"weight_decay\": 1e-4},\n",
        "    {\"learning_rate\": 3.5e-05, \"batch_size\": 6,  \"weight_decay\": 1e-4},\n",
        "    {\"learning_rate\": 3.5e-05, \"batch_size\": 7,  \"weight_decay\": 1.1e-4},\n",
        "    {\"learning_rate\": 3.5e-05, \"batch_size\": 6,  \"weight_decay\": 1.1e-4},\n",
        "    {\"learning_rate\": 3e-05,   \"batch_size\": 6,  \"weight_decay\": 1.1e-4},\n",
        "    {\"learning_rate\": 3.5e-05, \"batch_size\": 7,  \"weight_decay\": 9e-05},\n",
        "]\n",
        "\n",
        "# Top 10 HPO 2 loop 1 TPEsampler 81 kombinasi\n",
        "top10_hparams3 = [\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8,  \"weight_decay\": 1e-04, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6,  \"weight_decay\": 5e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 12, \"weight_decay\": 5e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6,  \"weight_decay\": 1e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 3e-05, \"batch_size\": 6,  \"weight_decay\": 1e-04, \"dropout_rate\": 0.10},\n",
        "]\n",
        "\n",
        "\n",
        "# top 10 HPO 2 loop 2 Gridsearch\n",
        "top10_hparams4 = [\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 2e-05, \"dropout_rate\": 0.12},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 7, \"weight_decay\": 1e-05, \"dropout_rate\": 0.12},\n",
        "    {\"learning_rate\": 6e-05, \"batch_size\": 6, \"weight_decay\": 2e-05, \"dropout_rate\": 0.12},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 7, \"weight_decay\": 1e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 7, \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "    {\"learning_rate\": 6e-05, \"batch_size\": 7, \"weight_decay\": 1e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 1e-05, \"dropout_rate\": 0.12},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 6, \"weight_decay\": 2e-05, \"dropout_rate\": 0.12},\n",
        "    {\"learning_rate\": 5e-05, \"batch_size\": 8, \"weight_decay\": 1e-05, \"dropout_rate\": 0.10},\n",
        "    {\"learning_rate\": 6e-05, \"batch_size\": 7, \"weight_decay\": 1e-05, \"dropout_rate\": 0.15},\n",
        "]\n",
        "\n",
        "# top10_hparams 1/2/3/4\n",
        "for i, params in enumerate(top10_hparams4):\n",
        "    print(f\"\\ Run Model {i+1}\")\n",
        "    # lr, bs, wd = params[\"learning_rate\"], params[\"batch_size\"], params[\"weight_decay\"] # without dropout config\n",
        "    lr, bs, wd, do = params[\"learning_rate\"], params[\"batch_size\"], params[\"weight_decay\"], params[\"dropout_rate\"]\n",
        "\n",
        "    # print(f\"Hyperparameters: \\nLr : {lr} \\nBs : {bs} \\nWd : {wd}\\n\") # without dropout config\n",
        "    print(f\"Hyperparameters: \\nLr : {lr} \\nBs : {bs} \\nWd : {wd} \\nDr : {do}\\n\")\n",
        "\n",
        "    config = T5Config.from_pretrained(\"t5-base\")\n",
        "    config.dropout_rate = do\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", config=config).to(device)\n",
        "    # model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device) # without dropout config\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8, weight_decay=wd)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bs)\n",
        "\n",
        "    # Path penyimpanan per model\n",
        "    model_path_i = os.path.join(MODEL_PATH, f'model_{i}')\n",
        "    tokenizer_path_i = os.path.join(TOKENIZER_PATH, f'tokenizer_{i}')\n",
        "    evaluation_path_i = os.path.join(EVALUATION_PATH, f'model_{i}')\n",
        "    os.makedirs(model_path_i, exist_ok=True)\n",
        "    os.makedirs(tokenizer_path_i, exist_ok=True)\n",
        "    os.makedirs(evaluation_path_i, exist_ok=True)\n",
        "\n",
        "    # Variabel training\n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience, counter = 3, 0\n",
        "    best_model_path, best_tokenizer_path = model_path_i, tokenizer_path_i\n",
        "\n",
        "    for epoch in range(20):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f'Training Epoch {epoch+1}'):\n",
        "            input_ids = batch['source_ids'].to(device)\n",
        "            attention_mask = batch['source_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                            labels=labels, decoder_attention_mask=decoder_attention_mask)\n",
        "            optimizer.zero_grad()\n",
        "            outputs.loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += outputs.loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}'):\n",
        "                input_ids = batch['source_ids'].to(device)\n",
        "                attention_mask = batch['source_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                decoder_attention_mask = batch['target_mask'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                labels=labels, decoder_attention_mask=decoder_attention_mask)\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_train = total_train_loss / len(train_loader)\n",
        "        avg_val = total_val_loss / len(val_loader)\n",
        "        train_losses.append(avg_train)\n",
        "        val_losses.append(avg_val)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train:.4f}, Val Loss = {avg_val:.4f}\")\n",
        "\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            model.save_pretrained(best_model_path)\n",
        "            tokenizer.save_pretrained(best_tokenizer_path)\n",
        "            print(f\"Best model saved at epoch {epoch+1} with val loss {avg_val:.4f}\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"No improvement. Patience counter: {counter}/{patience}\")\n",
        "\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    import os\n",
        "\n",
        "    # Decode Input (source/context) dari test_dataset\n",
        "    def decode_inputs_and_save(dataset, tokenizer, output_path):\n",
        "        decoded_inputs = []\n",
        "        for i in range(len(dataset)):\n",
        "            input_ids = dataset.inputs[i]['input_ids'].squeeze()\n",
        "            decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "            decoded_inputs.append(decoded_input)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            for line in decoded_inputs:\n",
        "                f.write(line.strip() + '\\n')\n",
        "        return decoded_inputs  # optionally returned for in-memory use\n",
        "\n",
        "    # Preprocess Context Only (clean and extract from raw input)\n",
        "    def preprocess_context(text):\n",
        "        try:\n",
        "            if 'context:' in text and 'answers:' in text:\n",
        "                context_part = text.split('context:')[1]\n",
        "                cleaned_context = context_part.split('answers:')[0].strip()\n",
        "                return f'\"{cleaned_context}\"'\n",
        "        except IndexError:\n",
        "            pass\n",
        "        return '\"\"'\n",
        "\n",
        "    def process_and_save_contexts(raw_input_path, processed_output_path):\n",
        "        with open(raw_input_path, 'r', encoding='utf-8') as infile:\n",
        "            raw_lines = infile.readlines()\n",
        "\n",
        "        cleaned_contexts = [preprocess_context(line) for line in raw_lines]\n",
        "\n",
        "        with open(processed_output_path, 'w', encoding='utf-8') as outfile:\n",
        "            for line in cleaned_contexts:\n",
        "                outfile.write(line.strip() + '\\n')\n",
        "        return cleaned_contexts\n",
        "\n",
        "    # Decode Target (reference/question) dari test_dataset\n",
        "    def decode_targets_and_save(dataset, tokenizer, output_path):\n",
        "        decoded_targets = []\n",
        "        for i in range(len(dataset)):\n",
        "            target_ids = dataset.targets[i]['input_ids'].squeeze()\n",
        "            decoded_target = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
        "            decoded_targets.append(decoded_target)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            for line in decoded_targets:\n",
        "                f.write(line.strip() + '\\n')\n",
        "        return decoded_targets\n",
        "\n",
        "    # Preprocess Target Text (for BLEU/METEOR/ROUGE)\n",
        "    def preprocess_target_text(text):\n",
        "        text = text.strip()\n",
        "        if text.endswith('?'):\n",
        "            text = text[:-1] + ' ?'\n",
        "        else:\n",
        "            text += ' ?'\n",
        "        text = text.replace(\"'s\", \" 's\").replace('  ', ' ')\n",
        "        if text.startswith('question:'):\n",
        "            text = text[9:].strip()\n",
        "        if text.startswith(','):\n",
        "            text = text[1:]\n",
        "        return text.lstrip()\n",
        "\n",
        "    def process_and_save_targets(raw_target_path, processed_output_path):\n",
        "        with open(raw_target_path, 'r', encoding='utf-8') as f:\n",
        "            raw_lines = f.readlines()\n",
        "\n",
        "        cleaned_targets = [preprocess_target_text(line) for line in raw_lines]\n",
        "\n",
        "        with open(processed_output_path, 'w', encoding='utf-8') as f:\n",
        "            for line in cleaned_targets:\n",
        "                f.write(line.strip() + '\\n')\n",
        "        return cleaned_targets\n",
        "\n",
        "    import os\n",
        "    import torch\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
        "    from nltk.translate.meteor_score import meteor_score\n",
        "    from rouge_score import rouge_scorer\n",
        "    import seaborn as sns\n",
        "    from evaluate import load\n",
        "    import re\n",
        "    import logging\n",
        "\n",
        "    # Konfigurasi logging\n",
        "    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')\n",
        "\n",
        "    # Path output per model\n",
        "    src_test_preds_path = os.path.join(model_path_i, 'src_test_preds.txt')\n",
        "    src_test_preds_processed_path = os.path.join(model_path_i, 'src_test_preds_processed.txt')\n",
        "    test_ref_path = os.path.join(model_path_i, 'test_ref.txt')\n",
        "    test_ref_processed_path = os.path.join(model_path_i, 'test_ref_processed.txt')\n",
        "\n",
        "    # 1. Decode & Simpan Input (berisi context+answer+instruksi)\n",
        "    decode_inputs_and_save(test_dataset, tokenizer, src_test_preds_path)\n",
        "    process_and_save_contexts(src_test_preds_path, src_test_preds_processed_path)\n",
        "\n",
        "    # 2. Decode & Simpan Target (berisi pertanyaan ground-truth)\n",
        "    decode_targets_and_save(test_dataset, tokenizer, test_ref_path)\n",
        "    process_and_save_targets(test_ref_path, test_ref_processed_path)\n",
        "\n",
        "    print(f\"Decoded input & target saved to model_{i} folder\")\n",
        "\n",
        "    print(f\"\\n{'='*30} EVALUATING MODEL #{i+1} {'='*30}\")\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_path_i)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path_i).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Generate predictions\n",
        "    generated_questions = []\n",
        "    for j in range(len(test_dataset)):\n",
        "        input_ids = test_dataset.inputs[j]['input_ids'].to(device)\n",
        "        attention_mask = test_dataset.inputs[j]['attention_mask'].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=target_max_len\n",
        "                )\n",
        "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_questions.append(question)\n",
        "\n",
        "    # Save predictions\n",
        "    preds_path = os.path.join(model_path_i, 'test_preds.txt')\n",
        "    with open(preds_path, 'w', encoding='utf-8') as f:\n",
        "        for q in generated_questions:\n",
        "            f.write(q.strip() + '\\n')\n",
        "\n",
        "    # Utility\n",
        "    def read_file(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return [line.strip() for line in f]\n",
        "\n",
        "    def write_file(file_path, data_list):\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            for line in data_list:\n",
        "                f.write(line.strip() + '\\n')\n",
        "\n",
        "    def preprocess_text_for_bleu(text):\n",
        "      text = text.replace('’', \"'\")  # Ganti curly apostrophe dengan apostrophe biasa\n",
        "      text = text.strip()\n",
        "      if text.endswith('?'):    # Versi pake spasi (kebih besar skor evaluasinya)\n",
        "          text = text[:-1] + ' ?'\n",
        "      else:\n",
        "          text = text + ' ?'\n",
        "      if \"'s\" in text:\n",
        "          text = text.replace(\"'s\", \" 's\")\n",
        "      text = text.replace('  ', ' ')\n",
        "      if text.startswith('question:'):\n",
        "          text = text[9:].strip()\n",
        "      if text.startswith(','):\n",
        "          text = text[1:]\n",
        "      return text.strip()\n",
        "\n",
        "    # Preprocess predictions\n",
        "    processed_preds = [preprocess_text_for_bleu(q) for q in generated_questions]\n",
        "    processed_preds_path = os.path.join(model_path_i, 'test_preds_processed.txt')\n",
        "    write_file(processed_preds_path, processed_preds)\n",
        "\n",
        "    # Load references (ref tetap sama dari test_ref_processed.txt)\n",
        "    references = read_file(test_ref_processed_path)\n",
        "    candidates = read_file(processed_preds_path)\n",
        "\n",
        "    references_tokenized = [ref.split() for ref in references]\n",
        "    candidates_tokenized = [cand.split() for cand in candidates]\n",
        "    list_of_references = [[ref] for ref in references_tokenized]\n",
        "\n",
        "    # METRICS\n",
        "    smoother = SmoothingFunction().method4\n",
        "    sacrebleu_score = sacrebleu.corpus_bleu(candidates, [references]).score\n",
        "\n",
        "    meteor_scores = []\n",
        "    for i, (r, c) in enumerate(zip(references, candidates)):\n",
        "        try:\n",
        "            r_tok = r.strip().split()\n",
        "            c_tok = c.strip().split()\n",
        "            score = meteor_score([r_tok], c_tok)  # Perbaikan di sini\n",
        "            meteor_scores.append(score)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"[{i}] Error calculating METEOR for r: {r} | c: {c}\")\n",
        "            logging.error(f\"[{i}] Exception: {str(e)}\")\n",
        "            meteor_scores.append(0.0)\n",
        "\n",
        "    rouge_l_scores = [rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(r, c)['rougeL'].fmeasure for r, c in zip(references, candidates)]\n",
        "\n",
        "    bleu_scores = []\n",
        "    for ref_tok, cand_tok in zip(references_tokenized, candidates_tokenized):\n",
        "        ref_list_tok = [ref_tok]\n",
        "        bleu_1 = sentence_bleu(ref_list_tok, cand_tok, weights=(1, 0, 0, 0), smoothing_function=smoother)\n",
        "        bleu_2 = sentence_bleu(ref_list_tok, cand_tok, weights=(0.5, 0.5, 0, 0), smoothing_function=smoother)\n",
        "        bleu_3 = sentence_bleu(ref_list_tok, cand_tok, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoother)\n",
        "        bleu_4 = sentence_bleu(ref_list_tok, cand_tok, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoother)\n",
        "        bleu_scores.append((bleu_1, bleu_2, bleu_3, bleu_4))\n",
        "\n",
        "    # Averages\n",
        "    avg_sentence_bleu_1 = sum(b[0] for b in bleu_scores) / len(bleu_scores) * 100\n",
        "    avg_sentence_bleu_2 = sum(b[1] for b in bleu_scores) / len(bleu_scores) * 100\n",
        "    avg_sentence_bleu_3 = sum(b[2] for b in bleu_scores) / len(bleu_scores) * 100\n",
        "    avg_sentence_bleu_4 = sum(b[3] for b in bleu_scores) / len(bleu_scores) * 100\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores) * 100\n",
        "    avg_rouge = sum(rouge_l_scores) / len(rouge_l_scores) * 100\n",
        "\n",
        "    corpus_bleu4 = corpus_bleu(list_of_references, candidates_tokenized, weights=(0.25, 0.25, 0.25, 0.25)) * 100\n",
        "\n",
        "    # Save Results\n",
        "    # Save summary\n",
        "    eval_txt_path = os.path.join(evaluation_path_i, 'evaluation_result.txt')\n",
        "    with open(eval_txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"SacreBLEU: {sacrebleu_score:.2f}\\n\")\n",
        "        f.write(f\"METEOR: {avg_meteor:.2f}\\n\")\n",
        "        f.write(f\"ROUGE-L: {avg_rouge:.2f}\\n\")\n",
        "        f.write(f\"Sentence BLEU-4: {avg_sentence_bleu_4:.2f}\\n\")\n",
        "        f.write(f\"Corpus BLEU-4: {corpus_bleu4:.2f}\\n\")\n",
        "    print(f\"Evaluation result saved to: {eval_txt_path}\")\n",
        "\n",
        "    # Path untuk simpan plot\n",
        "    evaluation_plot_path = os.path.join(evaluation_path_i, 'evaluation_result.png')\n",
        "\n",
        "    # Buat subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Primary Metrics\n",
        "    primary_labels = ['SacreBLEU', 'METEOR', 'ROUGE-L']\n",
        "    primary_scores = [sacrebleu_score, avg_meteor, avg_rouge]\n",
        "\n",
        "    bars1 = ax1.bar(primary_labels, primary_scores, color=['#e15759', '#af7aa1', '#76b7b2'])\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.set_title('Evaluation Metrics')\n",
        "    ax1.set_ylabel('Score (%)')\n",
        "\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "    # Sentence-Level BLEU Breakdown\n",
        "    sentence_labels = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']\n",
        "    sentence_scores = [avg_sentence_bleu_1, avg_sentence_bleu_2, avg_sentence_bleu_3, avg_sentence_bleu_4]\n",
        "\n",
        "    bars2 = ax2.bar(sentence_labels, sentence_scores, color=['#4e79a7', '#59a14f', '#9c755f', '#f28e2b'])\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.set_title('Sentence-Level BLEU Metrics Analysis')\n",
        "    ax2.set_ylabel('Score (%)')\n",
        "\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "    # Layout dan simpan\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(evaluation_plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Evaluation plot png saved to: {evaluation_plot_path}\")\n",
        "\n",
        "    del model, tokenizer, optimizer\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b2826B6q3YPY"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.13 (py310env)",
      "language": "python",
      "name": "py310env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}